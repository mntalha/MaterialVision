{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0503146",
   "metadata": {},
   "source": [
    "# CLIPP Evaluation: retrieval metrics\n",
    "\n",
    "This notebook loads the best CLIPP checkpoint, computes image and text embeddings on the validation set,\n",
    "and reports retrieval metrics (Top-1, Top-5, Top-10).\n",
    "\n",
    "Ensure you run this from the repository root so relative paths match (or update the paths below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7c7f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda, checkpoint: checkpoints/best_clipp.pth\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "from pathlib import Path\n",
    "import sys\n",
    "repo_root = Path('..').resolve()  # adjust if running from a different CWD\n",
    "sys.path.append(str(repo_root))\n",
    "\n",
    "CHECKPOINT_PATH = Path('checkpoints/best_clipp.pth')\n",
    "VAL_CSV = Path('../../data/alpaca_mbj_bandgap_test.csv')\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cuda' if __import__('torch').cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {DEVICE}, checkpoint: {CHECKPOINT_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6ed21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jipengsun/MaterialVision/models')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81c2f38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jipengsun/.conda/envs/clipp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-17 00:58:24,721 INFO: Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)\n",
      "2025-10-17 00:58:24,762 INFO: [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation examples: 1000\n"
     ]
    }
   ],
   "source": [
    "# Imports and model/dataset loading\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import CLIPP and ImageTextDataset from the training script\n",
    "from training import CLIPP, ImageTextDataset\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = CLIPP(proj_dim=256)\n",
    "device = torch.device(DEVICE)\n",
    "\n",
    "# Load checkpoint\n",
    "assert CHECKPOINT_PATH.exists(), f\"Checkpoint not found: {CHECKPOINT_PATH}\"\n",
    "ckpt = torch.load(str(CHECKPOINT_PATH), map_location=device)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load validation data\n",
    "val_df = pd.read_csv(VAL_CSV)\n",
    "val_ds = ImageTextDataset(val_df, tokenizer, train=False)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "print(f'Validation examples: {len(val_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36cbabc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed validation embeddings: images torch.Size([1000, 256]), texts torch.Size([1000, 256])\n"
     ]
    }
   ],
   "source": [
    "# Compute embeddings for entire validation set\n",
    "import torch\n",
    "val_img_embs_list = []\n",
    "val_txt_embs_list = []\n",
    "captions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        txts = batch['caption']\n",
    "        img_e, txt_e = model(images, input_ids, attention_mask)\n",
    "        val_img_embs_list.append(img_e.cpu())\n",
    "        val_txt_embs_list.append(txt_e.cpu())\n",
    "        captions.extend(txts)\n",
    "\n",
    "val_img_embs = torch.cat(val_img_embs_list, dim=0)\n",
    "val_txt_embs = torch.cat(val_txt_embs_list, dim=0)\n",
    "print(f'Computed validation embeddings: images {val_img_embs.shape}, texts {val_txt_embs.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcb181d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1: 0.1670, Top-5: 0.4040, Top-10: 0.5300\n"
     ]
    }
   ],
   "source": [
    "# Compute similarity scores and retrieval metrics\n",
    "import torch\n",
    "scores = val_txt_embs @ val_img_embs.T  # (N_text, N_image)\n",
    "\n",
    "# Top-k metrics as requested\n",
    "top1 = torch.mean((torch.argmax(scores, dim=1) == torch.arange(scores.shape[0], device=scores.device)).float()).item()\n",
    "top5 = torch.mean(\n",
    "    torch.tensor([\n",
    "        i in torch.topk(scores[i], 5).indices.tolist()\n",
    "        for i in range(scores.shape[0])\n",
    "    ], dtype=torch.float32, device=scores.device)\n",
    ").item()\n",
    "top10 = torch.mean(\n",
    "    torch.tensor([\n",
    "        i in torch.topk(scores[i], 10).indices.tolist()\n",
    "        for i in range(scores.shape[0])\n",
    "    ], dtype=torch.float32, device=scores.device)\n",
    ").item()\n",
    "\n",
    "print(f\"Top-1: {top1:.4f}, Top-5: {top5:.4f}, Top-10: {top10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1749ac33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed training embeddings: images torch.Size([5000, 256]), texts torch.Size([5000, 256])\n",
      "Train Top-1: 0.1930, Top-5: 0.4856, Top-10: 0.6360\n"
     ]
    }
   ],
   "source": [
    "# Compute retrieval metrics on the training set (Top-1 / Top-5 / Top-10)\n",
    "# WARNING: this computes an N x N similarity matrix and can be memory intensive for large datasets.\n",
    "TRAIN_CSV = Path('../../data/alpaca_mbj_bandgap_train.csv')\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "train_ds = ImageTextDataset(train_df, tokenizer, train=False)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "train_image_embs = []\n",
    "train_text_embs = []\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        img_e, txt_e = model(images, input_ids, attention_mask)\n",
    "        train_image_embs.append(img_e.cpu())\n",
    "        train_text_embs.append(txt_e.cpu())\n",
    "\n",
    "train_image_embeddings = torch.cat(train_image_embs, dim=0)\n",
    "train_text_embeddings = torch.cat(train_text_embs, dim=0)\n",
    "print(f'Computed training embeddings: images {train_image_embeddings.shape}, texts {train_text_embeddings.shape}')\n",
    "\n",
    "# compute similarity and retrieval metrics for the training set\n",
    "scores_train = train_text_embeddings @ train_image_embeddings.T  # (N_text, N_image)\n",
    "\n",
    "# Top-k metrics\n",
    "train_top1 = torch.mean((torch.argmax(scores_train, dim=1) == torch.arange(scores_train.shape[0], device=scores_train.device)).float()).item()\n",
    "train_top5 = torch.mean(\n",
    "    torch.tensor([\n",
    "        i in torch.topk(scores_train[i], 5).indices.tolist()\n",
    "        for i in range(scores_train.shape[0])\n",
    "    ], dtype=torch.float32, device=scores_train.device)\n",
    ").item()\n",
    "train_top10 = torch.mean(\n",
    "    torch.tensor([\n",
    "        i in torch.topk(scores_train[i], 10).indices.tolist()\n",
    "        for i in range(scores_train.shape[0])\n",
    "    ], dtype=torch.float32, device=scores_train.device)\n",
    ").item()\n",
    "\n",
    "print(f\"Train Top-1: {train_top1:.4f}, Top-5: {train_top5:.4f}, Top-10: {train_top10:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e4f146",
   "metadata": {},
   "source": [
    "# Embedding Visualization\n",
    "\n",
    "Let's visualize how well our model aligns the image and text embeddings in the shared space using t-SNE dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd24f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Combine image and text embeddings\n",
    "combined_embs = torch.cat([val_img_embs, val_txt_embs], dim=0)\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "combined_tsne = tsne.fit_transform(combined_embs.numpy())\n",
    "\n",
    "# Split back into image and text embeddings\n",
    "n = len(val_img_embs)\n",
    "img_tsne = combined_tsne[:n]\n",
    "txt_tsne = combined_tsne[n:]\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Plot all points\n",
    "plt.scatter(img_tsne[:, 0], img_tsne[:, 1], c='blue', label='Images', alpha=0.5, s=50)\n",
    "plt.scatter(txt_tsne[:, 0], txt_tsne[:, 1], c='red', label='Text', alpha=0.5, s=50)\n",
    "\n",
    "# Draw lines connecting corresponding pairs for a subset of examples\n",
    "num_examples = 10  # Number of example pairs to highlight\n",
    "random_indices = np.random.choice(n, num_examples, replace=False)\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    # Draw a line connecting the image-text pair\n",
    "    plt.plot([img_tsne[idx, 0], txt_tsne[idx, 0]], \n",
    "             [img_tsne[idx, 1], txt_tsne[idx, 1]], \n",
    "             'k-', alpha=0.3)\n",
    "    \n",
    "    # Add number labels\n",
    "    plt.annotate(f'Pair {i+1}', \n",
    "                xy=(img_tsne[idx, 0], img_tsne[idx, 1]),\n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "    plt.annotate(f'Pair {i+1}', \n",
    "                xy=(txt_tsne[idx, 0], txt_tsne[idx, 1]),\n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.title('t-SNE visualization of image and text embeddings (CLIPP-SciBERT)\\nValidation Set', fontsize=14)\n",
    "\n",
    "# Add text descriptions for a few example pairs\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.axis('off')\n",
    "# plt.text(0.1, 1.0, 'Example Pair Descriptions:', fontsize=12, fontweight='bold')\n",
    "# for i, idx in enumerate(random_indices[:5]):  # Show first 5 pairs\n",
    "#     plt.text(0.1, 0.9 - i*0.2, f'Pair {i+1}: {captions[idx][:100]}...', \n",
    "#              fontsize=10, wrap=True)\n",
    "\n",
    "plt.savefig('clipp_scibert_tsne.png', dpi=600)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
