{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0503146",
   "metadata": {},
   "source": [
    "# CLIPP Evaluation: retrieval metrics\n",
    "\n",
    "This notebook loads the best CLIPP checkpoint, computes image and text embeddings on the validation set,\n",
    "and reports retrieval metrics (Top-1, Top-5, Top-10).\n",
    "\n",
    "Ensure you run this from the repository root so relative paths match (or update the paths below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c7f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "from pathlib import Path\n",
    "import sys\n",
    "repo_root = Path('..').resolve()  # adjust if running from a different CWD\n",
    "sys.path.append(str(repo_root))\n",
    "\n",
    "CHECKPOINT_PATH = Path('models/baseCLIPP/checkpoints/best_clipp.pth')\n",
    "VAL_CSV = Path('data/alpaca_mbj_bandgap_val.csv')\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cuda' if __import__('torch').cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {DEVICE}, checkpoint: {CHECKPOINT_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c2f38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and model/dataset loading\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import CLIPP and ImageTextDataset from the training script\n",
    "from models.baseCLIPP.training import CLIPP, ImageTextDataset\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = CLIPP(proj_dim=256)\n",
    "device = torch.device(DEVICE)\n",
    "\n",
    "# Load checkpoint\n",
    "assert CHECKPOINT_PATH.exists(), f\"Checkpoint not found: {CHECKPOINT_PATH}\"\n",
    "ckpt = torch.load(str(CHECKPOINT_PATH), map_location=device)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load validation data\n",
    "val_df = pd.read_csv(VAL_CSV)\n",
    "val_ds = ImageTextDataset(val_df, tokenizer, train=False)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "print(f'Validation examples: {len(val_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cbabc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings for entire validation set\n",
    "import torch\n",
    "image_embs = []\n",
    "text_embs = []\n",
    "captions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        txts = batch['caption']\n",
    "        img_e, txt_e = model(images, input_ids, attention_mask)\n",
    "        image_embs.append(img_e.cpu())\n",
    "        text_embs.append(txt_e.cpu())\n",
    "        captions.extend(txts)\n",
    "\n",
    "image_embeddings = torch.cat(image_embs, dim=0)\n",
    "text_embeddings = torch.cat(text_embs, dim=0)\n",
    "print(f'Computed embeddings: images {image_embeddings.shape}, texts {text_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb181d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity scores and retrieval metrics\n",
    "import torch\n",
    "scores = text_embeddings @ image_embeddings.T  # (N_text, N_image)\n",
    "\n",
    "# Top-k metrics as requested\n",
    "top1 = torch.mean((torch.argmax(scores, dim=1) == torch.arange(scores.shape[0], device=scores.device)).float()).item()\n",
    "top5 = torch.mean(\n",
    "    torch.tensor([\n",
    "        i in torch.topk(scores[i], 5).indices.tolist()\n",
    "        for i in range(scores.shape[0])\n",
    "    ], dtype=torch.float32, device=scores.device)\n",
    ").item()\n",
    "top10 = torch.mean(\n",
    "    torch.tensor([\n",
    "        i in torch.topk(scores[i], 10).indices.tolist()\n",
    "        for i in range(scores.shape[0])\n",
    "    ], dtype=torch.float32, device=scores.device)\n",
    ").item()\n",
    "\n",
    "print(f\"Top-1: {top1:.4f}, Top-5: {top5:.4f}, Top-10: {top10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5105074",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- If the validation set is large, computing the full similarity matrix may be memory intensive. Consider computing in chunks or using smaller batches.\n",
    "- You can extend this notebook to compute per-class metrics, confusion matrices, or visualize nearest neighbor matches."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
