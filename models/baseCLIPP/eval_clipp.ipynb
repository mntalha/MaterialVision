{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0503146",
   "metadata": {},
   "source": [
    "# CLIPP Evaluation: retrieval metrics\n",
    "\n",
    "This notebook loads the best CLIPP checkpoint, computes image and text embeddings on the validation set,\n",
    "and reports retrieval metrics (Top-1, Top-5, Top-10).\n",
    "\n",
    "Ensure you run this from the repository root so relative paths match (or update the paths below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e7c7f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda, checkpoint: checkpoints/best_clipp.pth\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "from pathlib import Path\n",
    "import sys\n",
    "repo_root = Path('..').resolve()  # adjust if running from a different CWD\n",
    "sys.path.append(str(repo_root))\n",
    "\n",
    "CHECKPOINT_PATH = Path('checkpoints/best_clipp.pth')\n",
    "VAL_CSV = Path('../../data/alpaca_mbj_bandgap_test.csv')\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cuda' if __import__('torch').cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {DEVICE}, checkpoint: {CHECKPOINT_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b6ed21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jipengsun/MaterialVision/models')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81c2f38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 03:03:57,276 INFO: Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)\n",
      "2025-10-16 03:03:57,316 INFO: [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation examples: 1000\n"
     ]
    }
   ],
   "source": [
    "# Imports and model/dataset loading\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import CLIPP and ImageTextDataset from the training script\n",
    "from training import CLIPP, ImageTextDataset\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = CLIPP(proj_dim=256)\n",
    "device = torch.device(DEVICE)\n",
    "\n",
    "# Load checkpoint\n",
    "assert CHECKPOINT_PATH.exists(), f\"Checkpoint not found: {CHECKPOINT_PATH}\"\n",
    "ckpt = torch.load(str(CHECKPOINT_PATH), map_location=device)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load validation data\n",
    "val_df = pd.read_csv(VAL_CSV)\n",
    "val_ds = ImageTextDataset(val_df, tokenizer, train=False)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "print(f'Validation examples: {len(val_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36cbabc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed embeddings: images torch.Size([1000, 256]), texts torch.Size([1000, 256])\n"
     ]
    }
   ],
   "source": [
    "# Compute embeddings for entire validation set\n",
    "import torch\n",
    "image_embs = []\n",
    "text_embs = []\n",
    "captions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        txts = batch['caption']\n",
    "        img_e, txt_e = model(images, input_ids, attention_mask)\n",
    "        image_embs.append(img_e.cpu())\n",
    "        text_embs.append(txt_e.cpu())\n",
    "        captions.extend(txts)\n",
    "\n",
    "image_embeddings = torch.cat(image_embs, dim=0)\n",
    "text_embeddings = torch.cat(text_embs, dim=0)\n",
    "print(f'Computed embeddings: images {image_embeddings.shape}, texts {text_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcb181d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1: 0.1670, Top-5: 0.4040, Top-10: 0.5300\n"
     ]
    }
   ],
   "source": [
    "# Compute similarity scores and retrieval metrics\n",
    "import torch\n",
    "scores = text_embeddings @ image_embeddings.T  # (N_text, N_image)\n",
    "\n",
    "# Top-k metrics as requested\n",
    "top1 = torch.mean((torch.argmax(scores, dim=1) == torch.arange(scores.shape[0], device=scores.device)).float()).item()\n",
    "top5 = torch.mean(\n",
    "    torch.tensor([\n",
    "        i in torch.topk(scores[i], 5).indices.tolist()\n",
    "        for i in range(scores.shape[0])\n",
    "    ], dtype=torch.float32, device=scores.device)\n",
    ").item()\n",
    "top10 = torch.mean(\n",
    "    torch.tensor([\n",
    "        i in torch.topk(scores[i], 10).indices.tolist()\n",
    "        for i in range(scores.shape[0])\n",
    "    ], dtype=torch.float32, device=scores.device)\n",
    ").item()\n",
    "\n",
    "print(f\"Top-1: {top1:.4f}, Top-5: {top5:.4f}, Top-10: {top10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1749ac33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed training embeddings: images torch.Size([5000, 256]), texts torch.Size([5000, 256])\n",
      "Train Top-1: 0.1930, Top-5: 0.4856, Top-10: 0.6360\n",
      "Train Top-1: 0.1930, Top-5: 0.4856, Top-10: 0.6360\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Compute retrieval metrics on the training set (Top-1 / Top-5 / Top-10)\n",
    "# WARNING: this computes an N x N similarity matrix and can be memory intensive for large datasets.\n",
    "TRAIN_CSV = Path('../../data/alpaca_mbj_bandgap_train.csv')\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "train_ds = ImageTextDataset(train_df, tokenizer, train=False)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "train_image_embs = []\n",
    "train_text_embs = []\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        img_e, txt_e = model(images, input_ids, attention_mask)\n",
    "        train_image_embs.append(img_e.cpu())\n",
    "        train_text_embs.append(txt_e.cpu())\n",
    "\n",
    "train_image_embeddings = torch.cat(train_image_embs, dim=0)\n",
    "train_text_embeddings = torch.cat(train_text_embs, dim=0)\n",
    "print(f'Computed training embeddings: images {train_image_embeddings.shape}, texts {train_text_embeddings.shape}')\n",
    "\n",
    "# compute similarity and retrieval metrics for the training set\n",
    "scores_train = train_text_embeddings @ train_image_embeddings.T  # (N_text, N_image)\n",
    "\n",
    "# Top-k metrics\n",
    "train_top1 = torch.mean((torch.argmax(scores_train, dim=1) == torch.arange(scores_train.shape[0], device=scores_train.device)).float()).item()\n",
    "train_top5 = torch.mean(\n",
    "    torch.tensor([\n",
    "        i in torch.topk(scores_train[i], 5).indices.tolist()\n",
    "        for i in range(scores_train.shape[0])\n",
    "    ], dtype=torch.float32, device=scores_train.device)\n",
    ").item()\n",
    "train_top10 = torch.mean(\n",
    "    torch.tensor([\n",
    "        i in torch.topk(scores_train[i], 10).indices.tolist()\n",
    "        for i in range(scores_train.shape[0])\n",
    "    ], dtype=torch.float32, device=scores_train.device)\n",
    ").item()\n",
    "\n",
    "print(f\"Train Top-1: {train_top1:.4f}, Top-5: {train_top5:.4f}, Top-10: {train_top10:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
