{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02e0337f",
   "metadata": {},
   "source": [
    "# BLIP Model Evaluation\n",
    "\n",
    "This notebook evaluates a trained BLIP model on image-text retrieval tasks, computing Top-1/5/10 retrieval metrics on both training and validation sets. \n",
    "\n",
    "It includes:\n",
    "- Loading a trained checkpoint\n",
    "- Computing embeddings in memory-efficient chunks\n",
    "- Computing similarity matrices and retrieval metrics\n",
    "- Optional: visualizing example retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaba8e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jipengsun/.conda/envs/clipp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from transformers import AutoProcessor, BlipForImageTextRetrieval\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "# For mixed precision inference\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "415d8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class from train_blip.py\n",
    "class BlipForRetrieval(BlipForImageTextRetrieval):\n",
    "    def get_text_features(self,\n",
    "                          input_ids: torch.LongTensor,\n",
    "                          attention_mask: Optional[torch.LongTensor] = None,\n",
    "                          return_dict: Optional[bool] = None,\n",
    "                          ) -> torch.FloatTensor:\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        question_embeds = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        question_embeds = question_embeds[0] if not return_dict else question_embeds.last_hidden_state\n",
    "\n",
    "        text_feat = F.normalize(self.text_proj(question_embeds[:, 0, :]), dim=-1)\n",
    "\n",
    "        return text_feat\n",
    "\n",
    "    def get_image_features(\n",
    "            self,\n",
    "            pixel_values: torch.FloatTensor,\n",
    "            output_attentions: Optional[bool] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            return_dict: Optional[bool] = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "\n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        image_embeds = vision_outputs[0]\n",
    "\n",
    "        image_feat = F.normalize(self.vision_proj(image_embeds[:, 0, :]), dim=-1)\n",
    "\n",
    "        return image_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa8fb5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class from train_blip.py\n",
    "def list_to_image(img_list, size=224):\n",
    "    \"\"\"Convert a list to a 2D image of given size.\"\"\"\n",
    "    return np.array(json.loads(img_list)).reshape(size, size)\n",
    "\n",
    "class image_title_dataset():\n",
    "    def __init__(self, dataframe, processor):\n",
    "        self.text = dataframe[\"input\"]\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = list_to_image(self.dataframe[\"image\"][idx])\n",
    "        image = Image.fromarray(image).convert(\"RGB\") \n",
    "        image = self.processor(image)['pixel_values'][0]\n",
    "        text_ = self.text[idx]\n",
    "        id = self.dataframe[\"id\"][idx]\n",
    "        return image, text_, id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d95b4b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from epoch 10 with val_loss 0.7969\n"
     ]
    }
   ],
   "source": [
    "# Load model and processor\n",
    "model_name = \"Salesforce/blip-itm-large-coco\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = BlipForRetrieval.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load checkpoint\n",
    "ckpt_path = \"./checkpoints_blip/best_blip.pth\"\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "print(f\"Loaded checkpoint from epoch {ckpt['epoch']} with val_loss {ckpt['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c219adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv(\"../../data/alpaca_mbj_bandgap_train.csv\")\n",
    "val_df = pd.read_csv(\"../../data/alpaca_mbj_bandgap_test.csv\")\n",
    "\n",
    "train_ds = image_title_dataset(train_df, processor)\n",
    "val_ds = image_title_dataset(val_df, processor)\n",
    "\n",
    "# Create dataloaders with a reasonable batch size for GPU memory\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c53f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(model, dataloader, device, amp=True):\n",
    "    \"\"\"Compute image and text embeddings for a dataset in chunks.\"\"\"\n",
    "    model.eval()\n",
    "    all_img_embs = []\n",
    "    all_txt_embs = []\n",
    "    all_ids = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Computing embeddings'):\n",
    "            # Get image tensor, text list, and ids from batch\n",
    "            imgs, texts, ids = batch\n",
    "            imgs = imgs.to(device)\n",
    "            \n",
    "            # Process texts through processor\n",
    "            txts = processor(text=list(texts), padding=True, return_tensors='pt').to(device)\n",
    "            \n",
    "            # Use AMP if requested\n",
    "            if amp:\n",
    "                with autocast():\n",
    "                    img_feats = model.get_image_features(imgs)\n",
    "                    txt_feats = model.get_text_features(input_ids=txts['input_ids'], attention_mask=txts['attention_mask'])\n",
    "            else:\n",
    "                img_feats = model.get_image_features(imgs)\n",
    "                txt_feats = model.get_text_features(input_ids=txts['input_ids'], attention_mask=txts['attention_mask'])\n",
    "            \n",
    "            all_img_embs.append(img_feats.cpu())\n",
    "            all_txt_embs.append(txt_feats.cpu())\n",
    "            all_ids.append(ids)\n",
    "    \n",
    "    # Concatenate all embeddings and ids\n",
    "    img_embs = torch.cat(all_img_embs)\n",
    "    txt_embs = torch.cat(all_txt_embs)\n",
    "    ids = torch.cat(all_ids)\n",
    "    \n",
    "    # Sort by ID if needed\n",
    "    if ids is not None:\n",
    "        sort_idx = torch.argsort(ids)\n",
    "        img_embs = img_embs[sort_idx]\n",
    "        txt_embs = txt_embs[sort_idx]\n",
    "        ids = ids[sort_idx]\n",
    "    \n",
    "    return img_embs, txt_embs, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69184baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topk_chunked(img_embs, txt_embs, k=10, chunk_size=1024):\n",
    "    \"\"\"Compute Top-k retrieval metrics in memory-efficient chunks.\"\"\"\n",
    "    n = len(img_embs)\n",
    "    correct_at_k = {k: 0 for k in (1, 5, 10)}\n",
    "    \n",
    "    # Process in chunks to avoid OOM\n",
    "    for i in tqdm(range(0, n, chunk_size), desc='Computing metrics'):\n",
    "        chunk_img = img_embs[i:i+chunk_size].cuda()\n",
    "        chunk_txt = txt_embs[i:i+chunk_size].cuda()\n",
    "        \n",
    "        # Compute similarity matrix for this chunk\n",
    "        sim = chunk_img @ txt_embs.cuda().t()  # chunk_size x N\n",
    "        \n",
    "        # For each query in chunk, get Top-k indices\n",
    "        topk = torch.topk(sim, k=k, dim=1)[1]  # chunk_size x k\n",
    "        \n",
    "        # Count correct retrievals at different k\n",
    "        for k in correct_at_k.keys():\n",
    "            # For each query, is target in top-k?\n",
    "            correct = torch.any(topk[:, :k] == torch.arange(i, min(i+chunk_size, n), device=topk.device).unsqueeze(1), dim=1)\n",
    "            correct_at_k[k] += correct.sum().item()\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        del sim, topk\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Compute final metrics\n",
    "    metrics = {f'top{k}': count/n for k, count in correct_at_k.items()}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572c10ec",
   "metadata": {},
   "source": [
    "# Compute Validation Set Metrics\n",
    "\n",
    "Let's first evaluate on the validation set to get Top-1/5/10 retrieval metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "529eed1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings:   0%|          | 0/32 [00:00<?, ?it/s]/tmp/ipykernel_256722/1908032202.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/tmp/ipykernel_256722/1908032202.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Computing embeddings: 100%|██████████| 32/32 [00:18<00:00,  1.75it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing validation metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing metrics: 100%|██████████| 1/1 [00:00<00:00,  9.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "top1: 0.1730\n",
      "top5: 0.4290\n",
      "top10: 0.5520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute validation embeddings\n",
    "val_img_embs, val_txt_embs, val_ids = compute_embeddings(model, val_loader, device)\n",
    "\n",
    "# Compute validation metrics\n",
    "print(\"Computing validation metrics...\")\n",
    "val_metrics = compute_topk_chunked(val_img_embs, val_txt_embs)\n",
    "print(\"\\nValidation Metrics:\")\n",
    "for k, v in val_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654f6c8",
   "metadata": {},
   "source": [
    "# Compute Training Set Metrics\n",
    "\n",
    "Now let's compute the same metrics on the training set to check for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa17aa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings:   0%|          | 0/157 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_256722/1908032202.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/tmp/ipykernel_256722/1908032202.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Computing embeddings: 100%|██████████| 157/157 [01:22<00:00,  1.91it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing training metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing metrics: 100%|██████████| 5/5 [00:00<00:00, 84.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "top1: 0.3868\n",
      "top5: 0.7564\n",
      "top10: 0.8728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute training embeddings\n",
    "train_img_embs, train_txt_embs, train_ids = compute_embeddings(model, train_loader, device)\n",
    "\n",
    "# Compute training metrics\n",
    "print(\"Computing training metrics...\")\n",
    "train_metrics = compute_topk_chunked(train_img_embs, train_txt_embs)\n",
    "print(\"\\nTraining Metrics:\")\n",
    "for k, v in train_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a749adf2",
   "metadata": {},
   "source": [
    "# Example Retrievals\n",
    "\n",
    "Let's look at some example retrievals from the validation set to qualitatively assess the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8b0a03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 132\n",
      "================================================================================\n",
      "Query text: The chemical formula is BeVP2. The  mbj_bandgap value is 0.0.\n",
      "\n",
      "Top image→text retrievals:\n",
      "1. The chemical formula is BeVP2. The  mbj_bandgap value is 0.0.\n",
      "2. The chemical formula is ScAl. The  mbj_bandgap value is 0.0.\n",
      "3. The chemical formula is BePCl. The  mbj_bandgap value is 0.0.\n",
      "4. The chemical formula is FePS3. The  mbj_bandgap value is 0.0.\n",
      "5. The chemical formula is BeCoP2. The  mbj_bandgap value is 0.0.\n",
      "\n",
      "Top text→image retrievals:\n",
      "1. The chemical formula is BeVP2. The  mbj_bandgap value is 0.0.\n",
      "2. The chemical formula is ScBeNi2. The  mbj_bandgap value is 0.0.\n",
      "3. The chemical formula is BeCoP2. The  mbj_bandgap value is 0.0.\n",
      "4. The chemical formula is TiBeSe2. The  mbj_bandgap value is 0.0.\n",
      "5. The chemical formula is Cr3NiP4. The  mbj_bandgap value is 0.0.\n",
      "\n",
      "Example 8\n",
      "================================================================================\n",
      "Query text: The chemical formula is AlBr4. The  mbj_bandgap value is 0.0.\n",
      "\n",
      "Top image→text retrievals:\n",
      "1. The chemical formula is GaSeBr7. The  mbj_bandgap value is 2.974.\n",
      "2. The chemical formula is K3NbSe4. The  mbj_bandgap value is 2.124.\n",
      "3. The chemical formula is Rb2Se. The  mbj_bandgap value is 3.786.\n",
      "4. The chemical formula is PdSe2Br6. The  mbj_bandgap value is 1.244.\n",
      "5. The chemical formula is PdSe6Br2. The  mbj_bandgap value is 1.261.\n",
      "\n",
      "Top text→image retrievals:\n",
      "1. The chemical formula is Mg2HSe. The  mbj_bandgap value is 0.0.\n",
      "2. The chemical formula is AlBr4. The  mbj_bandgap value is 0.0.\n",
      "3. The chemical formula is MgSe. The  mbj_bandgap value is 4.28.\n",
      "4. The chemical formula is Sr3AsN. The  mbj_bandgap value is 1.027.\n",
      "5. The chemical formula is K2NiAs2. The  mbj_bandgap value is 0.234.\n",
      "\n",
      "Example 526\n",
      "================================================================================\n",
      "Query text: The chemical formula is Li2Pr2Si3. The  mbj_bandgap value is 0.0.\n",
      "\n",
      "Top image→text retrievals:\n",
      "1. The chemical formula is Ba4MgSc. The  mbj_bandgap value is 0.0.\n",
      "2. The chemical formula is DyAl3C3. The  mbj_bandgap value is 0.457.\n",
      "3. The chemical formula is Ba4YPt. The  mbj_bandgap value is 0.0.\n",
      "4. The chemical formula is BaH4O3. The  mbj_bandgap value is 7.072.\n",
      "5. The chemical formula is Sm3Th. The  mbj_bandgap value is 0.0.\n",
      "\n",
      "Top text→image retrievals:\n",
      "1. The chemical formula is DySi2. The  mbj_bandgap value is 0.0.\n",
      "2. The chemical formula is TbSi2Ni2. The  mbj_bandgap value is 0.0.\n",
      "3. The chemical formula is Al3Au. The  mbj_bandgap value is 0.0.\n",
      "4. The chemical formula is ThSi2Pd2. The  mbj_bandgap value is 0.0.\n",
      "5. The chemical formula is ThMn2Si2. The  mbj_bandgap value is 0.0.\n"
     ]
    }
   ],
   "source": [
    "def show_retrievals(query_idx, img_embs, txt_embs, texts, k=5):\n",
    "    \"\"\"Show top-k retrievals for a query image or text.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Get query embeddings\n",
    "        q_img = img_embs[query_idx].cuda()\n",
    "        q_txt = txt_embs[query_idx].cuda()\n",
    "        \n",
    "        # Compute similarities\n",
    "        img2txt = q_img @ txt_embs.cuda().t()\n",
    "        txt2img = q_txt @ img_embs.cuda().t()\n",
    "        \n",
    "        # Get top-k indices\n",
    "        _, i2t_idx = torch.topk(img2txt, k)\n",
    "        _, t2i_idx = torch.topk(txt2img, k)\n",
    "        \n",
    "        print(f\"Query text: {texts[query_idx]}\\n\")\n",
    "        print(\"Top image→text retrievals:\")\n",
    "        for i, idx in enumerate(i2t_idx.cpu().numpy(), 1):\n",
    "            print(f\"{i}. {texts[idx]}\")\n",
    "        \n",
    "        print(\"\\nTop text→image retrievals:\")\n",
    "        for i, idx in enumerate(t2i_idx.cpu().numpy(), 1):\n",
    "            print(f\"{i}. {texts[idx]}\")\n",
    "\n",
    "# Show retrievals for a few random validation examples\n",
    "for idx in np.random.choice(len(val_ds), 3):\n",
    "    print(f\"\\nExample {idx}\")\n",
    "    print(\"=\" * 80)\n",
    "    show_retrievals(idx, val_img_embs, val_txt_embs, val_df['input'].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
