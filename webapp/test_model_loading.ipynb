{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9514d9b",
   "metadata": {},
   "source": [
    "# MaterialVision Model Loading Demo\n",
    "\n",
    "This notebook demonstrates how to load and use the different vision-language models available in the MaterialVision project:\n",
    "\n",
    "- **CLIPP-SciBERT**: CLIPP model with SciBERT text encoder\n",
    "- **CLIPP-DistilBERT**: CLIPP model with DistilBERT text encoder  \n",
    "- **MobileCLIP**: Apple's MobileCLIP model\n",
    "- **BLIP**: Salesforce's BLIP model for image-text retrieval\n",
    "\n",
    "Each model has its own loading function that handles checkpoint loading, device placement, and provides a consistent interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf21e5",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5d74e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import importlib.util\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "# Since we're already in the webapp directory, we can import models.py directly\n",
    "# No need to add paths since models.py is in the same directory\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0e46b",
   "metadata": {},
   "source": [
    "## 2. Load Functions from External Files\n",
    "\n",
    "Now let's import the model loading functions from the `models.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09e3dd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding to path: /home/jipengsun/MaterialVision/models/CLIPP_allenai\n",
      "‚úÖ Successfully imported CLIPP SciBERT\n",
      "Adding to path: /home/jipengsun/MaterialVision/models/CLIPP_bert\n",
      "‚úÖ Successfully imported CLIPP DistilBERT\n",
      "Adding to path: /home/jipengsun/MaterialVision/models/Apple_MobileCLIP\n",
      "‚úÖ Successfully imported MobileCLIP\n",
      "Adding to path: /home/jipengsun/MaterialVision/models/Salesforce\n",
      "‚úÖ Successfully imported BLIP\n",
      "‚úÖ Successfully imported model loading functions:\n",
      "  - load_clipp_scibert\n",
      "  - load_clipp_distilbert\n",
      "  - load_mobileclip\n",
      "  - load_blip\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Import model loading functions from models.py\n",
    "    from models import (\n",
    "        load_clipp_scibert,\n",
    "        load_clipp_distilbert, \n",
    "        load_mobileclip,\n",
    "        load_blip\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Successfully imported model loading functions:\")\n",
    "    print(\"  - load_clipp_scibert\")\n",
    "    print(\"  - load_clipp_distilbert\")\n",
    "    print(\"  - load_mobileclip\") \n",
    "    print(\"  - load_blip\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing model functions: {e}\")\n",
    "    print(\"Make sure you're running this notebook from the MaterialVision root directory\")\n",
    "    print(\"and that the webapp/models.py file exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d53a7c",
   "metadata": {},
   "source": [
    "## 3. Call Loaded Functions with Sample Data\n",
    "\n",
    "Let's check for available checkpoints and demonstrate loading each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79c501ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ clipp_scibert: ../models/CLIPP_allenai/checkpoints/best_clipp.pth\n",
      "‚úÖ clipp_distilbert: ../models/CLIPP_bert/checkpoints/best_clipp_bert.pth\n",
      "‚úÖ mobileclip: ../models/Apple_MobileCLIP/checkpoints/best_clipp_apple.pth\n",
      "‚úÖ blip: ../models/Salesforce/checkpoints_blip/best_blip.pth\n",
      "\n",
      "Found 4 available model checkpoints.\n"
     ]
    }
   ],
   "source": [
    "# Define checkpoint paths (relative to webapp directory, go up one level to access models)\n",
    "checkpoint_paths = {\n",
    "    'clipp_scibert': '../models/CLIPP_allenai/checkpoints/best_clipp.pth',\n",
    "    'clipp_distilbert': '../models/CLIPP_bert/checkpoints/best_clipp_bert.pth', \n",
    "    'mobileclip': '../models/Apple_MobileCLIP/checkpoints/best_clipp_apple.pth',\n",
    "    'blip': '../models/Salesforce/checkpoints_blip/best_blip.pth'\n",
    "}\n",
    "\n",
    "# Check which checkpoints exist\n",
    "available_models = {}\n",
    "for model_name, path in checkpoint_paths.items():\n",
    "    full_path = Path(path)\n",
    "    if full_path.exists():\n",
    "        available_models[model_name] = str(full_path)\n",
    "        print(f\"‚úÖ {model_name}: {path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {model_name}: {path} (not found)\")\n",
    "\n",
    "print(f\"\\nFound {len(available_models)} available model checkpoints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9552a0",
   "metadata": {},
   "source": [
    "### 3.1 Load CLIPP-SciBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc1bc244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIPP-SciBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 23:18:23,539 INFO: Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)\n",
      "2025-11-09 23:18:23,581 INFO: [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CLIPP-SciBERT model loaded successfully!\n",
      "   Model device: cuda:0\n",
      "   Tokenizer type: BertTokenizerFast\n",
      "   Dataset type: ImageTextDataset\n",
      "sample input_ids: tensor([ 102,  158,  504,  170, 1240,  170, 3471,  244,  205,  244,  103,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "sample attention_mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Text embedding shape: torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "if 'clipp_scibert' in available_models:\n",
    "    try:\n",
    "        print(\"Loading CLIPP-SciBERT model...\")\n",
    "        clipp_scibert_model, clipp_scibert_tokenizer, clipp_scibert_dataset = load_clipp_scibert(\n",
    "            checkpoint_path=available_models['clipp_scibert'],\n",
    "            device=str(device)\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ CLIPP-SciBERT model loaded successfully!\")\n",
    "        print(f\"   Model device: {next(clipp_scibert_model.parameters()).device}\")\n",
    "        print(f\"   Tokenizer type: {type(clipp_scibert_tokenizer).__name__}\")\n",
    "        print(f\"   Dataset type: {type(clipp_scibert_dataset).__name__}\")        \n",
    "        # Test tokenization\n",
    "        sample_text = \"The chemical formula is UGe2Pt2. The mbj_bandgap value is 0.0.\"\n",
    "        caption, input_ids, attention_mask = clipp_scibert_dataset.prepare_caption(sample_text)\n",
    "        print(f\"sample input_ids: {input_ids}\")\n",
    "        print(f\"sample attention_mask: {attention_mask}\")\n",
    "\n",
    "        # Test text embedding\n",
    "        txt_emb = clipp_scibert_model.get_text_features(input_ids.view(1,-1).to(device), attention_mask.view(1,-1).to(device))\n",
    "        print(f\"Text embedding shape: {txt_emb.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading CLIPP-SciBERT: {e}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  CLIPP-SciBERT checkpoint not available, skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620bfb5b",
   "metadata": {},
   "source": [
    "### 3.2 Load CLIPP-DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ecd665e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIPP-DistilBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 23:18:40,711 INFO: Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "2025-11-09 23:18:40,756 INFO: [timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CLIPP-DistilBERT model loaded successfully!\n",
      "   Model device: cuda:0\n",
      "   Tokenizer type: DistilBertTokenizer\n",
      "   Dataset type: ImageTextDataset\n",
      "Text embedding shape: torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "if 'clipp_distilbert' in available_models:\n",
    "    try:\n",
    "        print(\"Loading CLIPP-DistilBERT model...\")\n",
    "        clipp_distilbert_model, clipp_distilbert_tokenizer, clipp_distilbert_dataset = load_clipp_distilbert(\n",
    "            checkpoint_path=available_models['clipp_distilbert'],\n",
    "            device=str(device)\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ CLIPP-DistilBERT model loaded successfully!\")\n",
    "        print(f\"   Model device: {next(clipp_distilbert_model.parameters()).device}\")\n",
    "        print(f\"   Tokenizer type: {type(clipp_distilbert_tokenizer).__name__}\")\n",
    "        print(f\"   Dataset type: {type(clipp_distilbert_dataset).__name__}\")\n",
    "\n",
    "        # Test tokenization\n",
    "        sample_text = \"The chemical formula is UGe2Pt2. The mbj_bandgap value is 0.0.\"\n",
    "        caption, input_ids, attention_mask = clipp_distilbert_dataset.prepare_caption(sample_text)\n",
    "        embeddings = clipp_distilbert_model.get_text_features(input_ids.view(1,-1).to(device), attention_mask.view(1,-1).to(device))\n",
    "        print(f\"Text embedding shape: {embeddings.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading CLIPP-DistilBERT: {e}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  CLIPP-DistilBERT checkpoint not available, skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488de851",
   "metadata": {},
   "source": [
    "### 3.3 Load MobileCLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e961f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 23:18:49,610 INFO: Loaded MobileCLIP-S2 model config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MobileCLIP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 23:18:51,001 INFO: Loading pretrained MobileCLIP-S2 weights (datacompdr).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MobileCLIP model loaded successfully!\n",
      "   Model device: cuda:0\n",
      "   Tokenizer type: <class 'open_clip.tokenizer.SimpleTokenizer'>\n",
      "   Dataset type: ImageTextDataset\n",
      "Text embedding shape: torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "if 'mobileclip' in available_models:\n",
    "    try:\n",
    "        print(\"Loading MobileCLIP model...\")\n",
    "        mobileclip_model, mobileclip_tokenizer, mobileclip_dataset = load_mobileclip(\n",
    "            checkpoint_path=available_models['mobileclip'],\n",
    "            device=str(device)\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ MobileCLIP model loaded successfully!\")\n",
    "        print(f\"   Model device: {next(mobileclip_model.parameters()).device}\")\n",
    "        print(f\"   Tokenizer type: {type(mobileclip_tokenizer)}\")\n",
    "        print(f\"   Dataset type: {type(mobileclip_dataset).__name__}\")\n",
    "\n",
    "        # Test tokenization (MobileCLIP uses different tokenization)\n",
    "        sample_text = \"The chemical formula is UGe2Pt2. The mbj_bandgap value is 0.0.\"\n",
    "        caption, text_tokens = mobileclip_dataset.prepare_caption(sample_text)\n",
    "        embeddings = mobileclip_model.get_text_features(text_tokens.to(device))\n",
    "        print(f\"Text embedding shape: {embeddings.shape}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading MobileCLIP: {e}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  MobileCLIP checkpoint not available, skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eba7318",
   "metadata": {},
   "source": [
    "### 3.4 Load BLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d588ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BLIP model...\n",
      "‚úÖ BLIP model loaded successfully!\n",
      "   Model device: cuda:0\n",
      "   Processor type: BlipProcessor\n",
      "   Dataset type: ImageTextDataset\n",
      "Text embedding shape: torch.Size([17, 256])\n"
     ]
    }
   ],
   "source": [
    "if 'blip' in available_models:\n",
    "    try:\n",
    "        print(\"Loading BLIP model...\")\n",
    "        blip_model, blip_processor, blip_dataset = load_blip(\n",
    "            checkpoint_path=available_models['blip'],\n",
    "            device=str(device)\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ BLIP model loaded successfully!\")\n",
    "        print(f\"   Model device: {next(blip_model.parameters()).device}\")\n",
    "        print(f\"   Processor type: {type(blip_processor).__name__}\")\n",
    "        print(f\"   Dataset type: {type(blip_dataset).__name__}\")\n",
    "        \n",
    "        # Test text processing\n",
    "        sample_text = \"The chemical formula is UGe2Pt2. The mbj_bandgap value is 0.0.\"\n",
    "        caption, input_ids, attention_mask = blip_dataset.prepare_caption(sample_text)\n",
    "        embeddings = blip_model.get_text_features(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n",
    "        print(f\"Text embedding shape: {embeddings.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading BLIP: {e}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  BLIP checkpoint not available, skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8288dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1 U 2 Ge 2 Pt 0.0', torch.Size([17, 3]), torch.Size([17, 256]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption, attention_mask.shape, embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11522bf7",
   "metadata": {},
   "source": [
    "## 4. Display Function Results\n",
    "\n",
    "Let's demonstrate how to use the loaded models for text encoding and feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dcb769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample material science texts\n",
    "sample_texts = [\n",
    "    \"Silicon dioxide has excellent optical properties\",\n",
    "    \"Graphene exhibits high electrical conductivity\", \n",
    "    \"Titanium dioxide is a versatile photocatalyst\",\n",
    "    \"Perovskite materials for solar cell applications\"\n",
    "]\n",
    "\n",
    "print(\"üî¨ Testing text feature extraction with loaded models:\\n\")\n",
    "\n",
    "# Test each loaded model\n",
    "loaded_models = []\n",
    "\n",
    "# Check CLIPP-SciBERT\n",
    "if 'clipp_scibert' in available_models and 'clipp_scibert_model' in locals():\n",
    "    loaded_models.append(('CLIPP-SciBERT', clipp_scibert_model, clipp_scibert_tokenizer))\n",
    "\n",
    "# Check CLIPP-DistilBERT  \n",
    "if 'clipp_distilbert' in available_models and 'clipp_distilbert_model' in locals():\n",
    "    loaded_models.append(('CLIPP-DistilBERT', clipp_distilbert_model, clipp_distilbert_tokenizer))\n",
    "\n",
    "# Check MobileCLIP\n",
    "if 'mobileclip' in available_models and 'mobileclip_model' in locals():\n",
    "    loaded_models.append(('MobileCLIP', mobileclip_model, mobileclip_tokenizer))\n",
    "\n",
    "# Check BLIP\n",
    "if 'blip' in available_models and 'blip_model' in locals():\n",
    "    loaded_models.append(('BLIP', blip_model, blip_processor))\n",
    "\n",
    "print(f\"Testing with {len(loaded_models)} successfully loaded models:\")\n",
    "for name, _, _ in loaded_models:\n",
    "    print(f\"  ‚úì {name}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c2e6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text feature extraction for each model\n",
    "for model_name, model, tokenizer_or_processor in loaded_models:\n",
    "    print(f\"üìù Testing {model_name}:\")\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            if model_name == 'BLIP':\n",
    "                # BLIP uses processor\n",
    "                processed = tokenizer_or_processor(text=sample_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "                # Move tensors to device\n",
    "                for key in processed:\n",
    "                    if isinstance(processed[key], torch.Tensor):\n",
    "                        processed[key] = processed[key].to(device)\n",
    "                features = model.get_text_features(**processed)\n",
    "            \n",
    "            elif model_name == 'MobileCLIP':\n",
    "                # MobileCLIP uses different tokenization\n",
    "                tokens = tokenizer_or_processor(sample_texts).to(device)\n",
    "                features = model.get_text_features(tokens)\n",
    "            \n",
    "            else:\n",
    "                # CLIPP models use standard tokenization\n",
    "                tokens = tokenizer_or_processor(sample_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "                # Move tensors to device\n",
    "                for key in tokens:\n",
    "                    tokens[key] = tokens[key].to(device)\n",
    "                features = model.get_text_features(tokens['input_ids'], tokens['attention_mask'])\n",
    "            \n",
    "            print(f\"   ‚úÖ Text features shape: {features.shape}\")\n",
    "            print(f\"   üìä Feature statistics:\")\n",
    "            print(f\"      Mean: {features.mean().item():.4f}\")\n",
    "            print(f\"      Std:  {features.std().item():.4f}\")\n",
    "            print(f\"      Min:  {features.min().item():.4f}\")\n",
    "            print(f\"      Max:  {features.max().item():.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668da15d",
   "metadata": {},
   "source": [
    "## 5. Error Handling for Function Calls\n",
    "\n",
    "Let's demonstrate robust error handling when working with these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43a90c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_model_loading(model_name, load_function, checkpoint_path, device):\n",
    "    \"\"\"\n",
    "    Safely load a model with comprehensive error handling.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model for logging\n",
    "        load_function: Function to load the model\n",
    "        checkpoint_path: Path to model checkpoint\n",
    "        device: Device to load model on\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success, model_data, error_message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üîÑ Attempting to load {model_name}...\")\n",
    "        \n",
    "        # Check if checkpoint exists\n",
    "        if not Path(checkpoint_path).exists():\n",
    "            raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "        \n",
    "        # Check if checkpoint is readable\n",
    "        if not os.access(checkpoint_path, os.R_OK):\n",
    "            raise PermissionError(f\"Cannot read checkpoint: {checkpoint_path}\")\n",
    "        \n",
    "        # Attempt to load the model\n",
    "        model_data = load_function(checkpoint_path, device)\n",
    "        \n",
    "        # Validate model data\n",
    "        if not model_data or len(model_data) != 3:\n",
    "            raise ValueError(\"Invalid model data returned from load function\")\n",
    "        \n",
    "        model, tokenizer, dataset = model_data\n",
    "        \n",
    "        # Basic validation\n",
    "        if model is None:\n",
    "            raise ValueError(\"Model is None\")\n",
    "        \n",
    "        if tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer/Processor is None\")\n",
    "            \n",
    "        print(f\"‚úÖ {model_name} loaded successfully!\")\n",
    "        return True, model_data, None\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        error_msg = f\"File not found: {e}\"\n",
    "        print(f\"‚ùå {model_name}: {error_msg}\")\n",
    "        return False, None, error_msg\n",
    "        \n",
    "    except PermissionError as e:\n",
    "        error_msg = f\"Permission error: {e}\"\n",
    "        print(f\"‚ùå {model_name}: {error_msg}\")\n",
    "        return False, None, error_msg\n",
    "        \n",
    "    except ImportError as e:\n",
    "        error_msg = f\"Import error (missing dependencies): {e}\"\n",
    "        print(f\"‚ùå {model_name}: {error_msg}\")\n",
    "        return False, None, error_msg\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        error_msg = f\"Runtime error (possibly CUDA/memory): {e}\"\n",
    "        print(f\"‚ùå {model_name}: {error_msg}\")\n",
    "        return False, None, error_msg\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Unexpected error: {type(e).__name__}: {e}\"\n",
    "        print(f\"‚ùå {model_name}: {error_msg}\")\n",
    "        return False, None, error_msg\n",
    "\n",
    "# Test safe loading with a non-existent checkpoint\n",
    "print(\"üß™ Testing error handling with invalid checkpoint:\")\n",
    "success, data, error = safe_model_loading(\n",
    "    \"Test Model\", \n",
    "    load_clipp_scibert, \n",
    "    \"non_existent_checkpoint.pth\", \n",
    "    str(device)\n",
    ")\n",
    "print(f\"   Success: {success}\")\n",
    "print(f\"   Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046e081",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "\n",
    "1. **Import model loading functions** from the `models.py` file (located in the same webapp directory)\n",
    "2. **Check for available checkpoints** in the `../models/` directory and handle missing files gracefully\n",
    "3. **Load each model type** (CLIPP-SciBERT, CLIPP-DistilBERT, MobileCLIP, BLIP) with proper error handling\n",
    "4. **Extract text features** using the loaded models with sample material science texts\n",
    "5. **Load validation data** from `../../data/alpaca_mbj_bandgap_test.csv`\n",
    "6. **Generate text and image embeddings** for all loaded models on the validation set\n",
    "7. **Save embeddings** to the `./embeddings/` directory in multiple formats (pickle, numpy, text)\n",
    "8. **Compute retrieval metrics** (text-to-image and image-to-text) for performance evaluation\n",
    "9. **Implement robust error handling** for model loading and embedding generation operations\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- Each model has a consistent interface through the `get_text_features()` and `get_image_features()` methods\n",
    "- Different models use different tokenization approaches (AutoTokenizer vs open_clip tokenizer vs Processor)\n",
    "- Embeddings are saved in an organized directory structure under `./embeddings/[model_name]/`\n",
    "- Retrieval metrics provide quantitative comparison between different models\n",
    "- Proper error handling is crucial when working with large models and checkpoints\n",
    "- All models can be used on both CPU and GPU devices\n",
    "- The notebook is designed to run from the webapp directory with relative paths to the models\n",
    "\n",
    "### Generated Files:\n",
    "\n",
    "- `./embeddings/[model_name]/validation_embeddings.pkl` - Complete embeddings data\n",
    "- `./embeddings/[model_name]/text_embeddings.npy` - Text embeddings as numpy array\n",
    "- `./embeddings/[model_name]/image_embeddings.npy` - Image embeddings as numpy array  \n",
    "- `./embeddings/[model_name]/captions.txt` - Text captions\n",
    "- `./embeddings/retrieval_metrics.pkl` - Computed retrieval metrics for all models\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Use embeddings for similarity search between text and image queries\n",
    "- Implement embedding-based material property prediction\n",
    "- Compare embedding quality across different models\n",
    "- Fine-tune models on domain-specific material science data\n",
    "- Build web applications using the saved embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd19415",
   "metadata": {},
   "source": [
    "## 6. Load Validation Data and Generate Embeddings\n",
    "\n",
    "Now let's load the validation data and use each model to generate text and image embeddings, saving them to the embeddings folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17a11244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CSV path: ../../data/alpaca_mbj_bandgap_test.csv\n",
      "Validation CSV exists: False\n",
      "Embeddings directory: embeddings\n",
      "‚ùå Validation CSV not found at ../../data/alpaca_mbj_bandgap_test.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Define validation data path\n",
    "VAL_CSV = Path('../../data/alpaca_mbj_bandgap_test.csv')\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create embeddings directory\n",
    "embeddings_dir = Path('./embeddings')\n",
    "embeddings_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Validation CSV path: {VAL_CSV}\")\n",
    "print(f\"Validation CSV exists: {VAL_CSV.exists()}\")\n",
    "print(f\"Embeddings directory: {embeddings_dir}\")\n",
    "\n",
    "# Load validation data\n",
    "if VAL_CSV.exists():\n",
    "    val_df = pd.read_csv(VAL_CSV)\n",
    "    print(f\"‚úÖ Loaded validation data with {len(val_df)} samples\")\n",
    "    print(f\"   Columns: {list(val_df.columns)}\")\n",
    "    print(f\"   Sample columns preview:\")\n",
    "    for col in val_df.columns[:5]:  # Show first 5 columns\n",
    "        print(f\"     {col}: {val_df[col].iloc[0] if len(str(val_df[col].iloc[0])) < 50 else str(val_df[col].iloc[0])[:50] + '...'}\")\n",
    "else:\n",
    "    print(f\"‚ùå Validation CSV not found at {VAL_CSV}\")\n",
    "    val_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2875c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate unified embeddings for each model\n",
    "def generate_embeddings_for_model(model_name, model, tokenizer_or_processor, dataset_class, val_df, device):\n",
    "    \"\"\"\n",
    "    Generate unified embeddings that combine both text and image information into single embeddings.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model (for saving files)\n",
    "        model: The loaded model\n",
    "        tokenizer_or_processor: Tokenizer or processor for the model\n",
    "        dataset_class: Dataset class for creating data loader\n",
    "        val_df: Validation dataframe\n",
    "        device: Device to run on\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (unified_embeddings, sample_ids, captions, text_embeddings, image_embeddings)\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîÑ Generating unified embeddings for {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Create dataset and dataloader based on model type\n",
    "        if model_name == 'MobileCLIP':\n",
    "            # For MobileCLIP, need to import the specific preprocessing\n",
    "            sys.path.append('../models/Apple_MobileCLIP')\n",
    "            import open_clip\n",
    "            \n",
    "            # Load MobileCLIP-S2 preprocessor  \n",
    "            _, _, preprocess_s2 = open_clip.create_model_and_transforms('MobileCLIP-S2', pretrained='datacompdr')\n",
    "            \n",
    "            val_dataset = dataset_class(val_df, tokenizer_or_processor, preprocess_s2, train=False)\n",
    "            \n",
    "        elif model_name == 'BLIP':\n",
    "            # BLIP uses a processor for both text and images\n",
    "            val_dataset = dataset_class(val_df, tokenizer_or_processor, train=False)\n",
    "            \n",
    "        else:\n",
    "            # CLIPP models use standard tokenizer + transform\n",
    "            # We'll need to define a transform for images\n",
    "            from torchvision import transforms\n",
    "            \n",
    "            # Standard image preprocessing for CLIPP models\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "            val_dataset = dataset_class(val_df, tokenizer_or_processor, transform, train=False)\n",
    "        \n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "        \n",
    "        # Generate embeddings with proper tracking\n",
    "        text_embeddings_list = []\n",
    "        image_embeddings_list = []\n",
    "        captions = []\n",
    "        sample_ids = []  # Track unique IDs for each sample\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            global_sample_idx = 0  # Global counter for unique IDs\n",
    "            \n",
    "            for i, batch in enumerate(val_loader):\n",
    "                print(f\"   Processing batch {i+1}/{len(val_loader)}\", end='\\r')\n",
    "                \n",
    "                if model_name == 'MobileCLIP':\n",
    "                    # MobileCLIP specific processing\n",
    "                    images = batch['image'].to(device)\n",
    "                    text_tokens = batch['text_tokens'].to(device)\n",
    "                    texts = batch['caption']\n",
    "                    \n",
    "                    # Get embeddings using the model's forward method\n",
    "                    img_emb, txt_emb = model(images, text_tokens)\n",
    "                    \n",
    "                elif model_name == 'BLIP':\n",
    "                    # BLIP specific processing\n",
    "                    images = batch['image'].to(device)\n",
    "                    texts = batch['caption']\n",
    "                    \n",
    "                    # Process text\n",
    "                    text_inputs = tokenizer_or_processor(text=texts, return_tensors='pt', padding=True, truncation=True)\n",
    "                    for key in text_inputs:\n",
    "                        if isinstance(text_inputs[key], torch.Tensor):\n",
    "                            text_inputs[key] = text_inputs[key].to(device)\n",
    "                    \n",
    "                    # Get embeddings\n",
    "                    img_emb = model.get_image_features(images)\n",
    "                    txt_emb = model.get_text_features(**text_inputs)\n",
    "                    \n",
    "                else:\n",
    "                    # CLIPP models\n",
    "                    images = batch['image'].to(device)\n",
    "                    texts = batch['caption']\n",
    "                    \n",
    "                    # Tokenize text\n",
    "                    text_inputs = tokenizer_or_processor(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "                    for key in text_inputs:\n",
    "                        text_inputs[key] = text_inputs[key].to(device)\n",
    "                    \n",
    "                    # Get embeddings\n",
    "                    img_emb = model.get_image_features(images)\n",
    "                    txt_emb = model.get_text_features(text_inputs['input_ids'], text_inputs['attention_mask'])\n",
    "                \n",
    "                # Collect embeddings and metadata with proper ordering\n",
    "                batch_size = len(texts)\n",
    "                batch_ids = list(range(global_sample_idx, global_sample_idx + batch_size))\n",
    "                \n",
    "                text_embeddings_list.append(txt_emb.cpu())\n",
    "                image_embeddings_list.append(img_emb.cpu())\n",
    "                captions.extend(texts)\n",
    "                sample_ids.extend(batch_ids)\n",
    "                \n",
    "                global_sample_idx += batch_size\n",
    "        \n",
    "        # Concatenate all embeddings\n",
    "        text_embeddings = torch.cat(text_embeddings_list, dim=0)\n",
    "        image_embeddings = torch.cat(image_embeddings_list, dim=0)\n",
    "        \n",
    "        # Create unified embeddings by combining text and image embeddings\n",
    "        print(f\"\\n   üîÑ Creating unified embeddings...\")\n",
    "        \n",
    "        # Method 1: Simple concatenation\n",
    "        unified_embeddings_concat = torch.cat([text_embeddings, image_embeddings], dim=1)\n",
    "        \n",
    "        # Method 2: Weighted average (you can adjust weights)\n",
    "        text_weight = 0.5\n",
    "        image_weight = 0.5\n",
    "        \n",
    "        # Ensure both embeddings have same dimension by projecting to common space if needed\n",
    "        if text_embeddings.shape[1] != image_embeddings.shape[1]:\n",
    "            # Project to smaller dimension\n",
    "            target_dim = min(text_embeddings.shape[1], image_embeddings.shape[1])\n",
    "            \n",
    "            # Simple linear projection (you could use learned projections too)\n",
    "            if text_embeddings.shape[1] > target_dim:\n",
    "                text_proj = text_embeddings[:, :target_dim]\n",
    "            else:\n",
    "                text_proj = torch.cat([text_embeddings, torch.zeros(text_embeddings.shape[0], target_dim - text_embeddings.shape[1])], dim=1)\n",
    "                \n",
    "            if image_embeddings.shape[1] > target_dim:\n",
    "                image_proj = image_embeddings[:, :target_dim]\n",
    "            else:\n",
    "                image_proj = torch.cat([image_embeddings, torch.zeros(image_embeddings.shape[0], target_dim - image_embeddings.shape[1])], dim=1)\n",
    "        else:\n",
    "            text_proj = text_embeddings\n",
    "            image_proj = image_embeddings\n",
    "        \n",
    "        unified_embeddings_avg = text_weight * text_proj + image_weight * image_proj\n",
    "        \n",
    "        # Method 3: Element-wise operations\n",
    "        unified_embeddings_multiply = text_proj * image_proj  # Element-wise multiplication\n",
    "        unified_embeddings_max = torch.max(text_proj, image_proj)  # Element-wise maximum\n",
    "        \n",
    "        print(f\"   ‚úÖ Generated unified embeddings:\")\n",
    "        print(f\"      Text embeddings: {text_embeddings.shape}\")\n",
    "        print(f\"      Image embeddings: {image_embeddings.shape}\")\n",
    "        print(f\"      Unified (concat): {unified_embeddings_concat.shape}\")\n",
    "        print(f\"      Unified (avg): {unified_embeddings_avg.shape}\")\n",
    "        print(f\"      Unified (multiply): {unified_embeddings_multiply.shape}\")\n",
    "        print(f\"      Unified (max): {unified_embeddings_max.shape}\")\n",
    "        print(f\"      Number of captions: {len(captions)}\")\n",
    "        print(f\"      Number of sample IDs: {len(sample_ids)}\")\n",
    "        print(f\"      Sample ID range: {min(sample_ids)} - {max(sample_ids)}\")\n",
    "        \n",
    "        # Use concatenation as the default unified embedding (most comprehensive)\n",
    "        unified_embeddings = unified_embeddings_concat\n",
    "        \n",
    "        # Verify pairing consistency\n",
    "        assert len(captions) == len(sample_ids) == unified_embeddings.shape[0] == text_embeddings.shape[0] == image_embeddings.shape[0], \\\n",
    "            f\"Inconsistent lengths: captions={len(captions)}, ids={len(sample_ids)}, \" \\\n",
    "            f\"unified_emb={unified_embeddings.shape[0]}, text_emb={text_embeddings.shape[0]}, img_emb={image_embeddings.shape[0]}\"\n",
    "        \n",
    "        # Save embeddings with proper pairing information\n",
    "        model_embeddings_dir = embeddings_dir / model_name.lower().replace('-', '_')\n",
    "        model_embeddings_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create comprehensive embeddings data with unified embeddings\n",
    "        embeddings_data = {\n",
    "            'unified_embeddings': unified_embeddings.numpy(),  # Main unified embedding\n",
    "            'unified_embeddings_concat': unified_embeddings_concat.numpy(),\n",
    "            'unified_embeddings_avg': unified_embeddings_avg.numpy(), \n",
    "            'unified_embeddings_multiply': unified_embeddings_multiply.numpy(),\n",
    "            'unified_embeddings_max': unified_embeddings_max.numpy(),\n",
    "            'text_embeddings': text_embeddings.numpy(),  # Keep originals for analysis\n",
    "            'image_embeddings': image_embeddings.numpy(),\n",
    "            'captions': captions,\n",
    "            'sample_ids': sample_ids,\n",
    "            'model_name': model_name,\n",
    "            'fusion_info': {\n",
    "                'default_method': 'concatenation',\n",
    "                'text_weight': text_weight,\n",
    "                'image_weight': image_weight,\n",
    "                'original_text_dim': text_embeddings.shape[1],\n",
    "                'original_image_dim': image_embeddings.shape[1],\n",
    "                'unified_dim': unified_embeddings.shape[1]\n",
    "            },\n",
    "            'pairing_info': {\n",
    "                'description': 'unified_embeddings[i] combines text and image info for captions[i] and sample_ids[i]',\n",
    "                'total_pairs': len(sample_ids),\n",
    "                'embedding_dim_unified': unified_embeddings.shape[1],\n",
    "                'embedding_dim_text': text_embeddings.shape[1],\n",
    "                'embedding_dim_image': image_embeddings.shape[1]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save as pickle with all metadata\n",
    "        pickle_path = model_embeddings_dir / 'validation_embeddings.pkl'\n",
    "        with open(pickle_path, 'wb') as f:\n",
    "            pickle.dump(embeddings_data, f)\n",
    "        \n",
    "        # Save different versions of unified embeddings\n",
    "        np.save(model_embeddings_dir / 'unified_embeddings.npy', unified_embeddings.numpy())\n",
    "        np.save(model_embeddings_dir / 'unified_embeddings_concat.npy', unified_embeddings_concat.numpy())\n",
    "        np.save(model_embeddings_dir / 'unified_embeddings_avg.npy', unified_embeddings_avg.numpy())\n",
    "        np.save(model_embeddings_dir / 'unified_embeddings_multiply.npy', unified_embeddings_multiply.numpy())\n",
    "        np.save(model_embeddings_dir / 'unified_embeddings_max.npy', unified_embeddings_max.numpy())\n",
    "        \n",
    "        # Save original embeddings for comparison\n",
    "        np.save(model_embeddings_dir / 'text_embeddings.npy', text_embeddings.numpy())\n",
    "        np.save(model_embeddings_dir / 'image_embeddings.npy', image_embeddings.numpy())\n",
    "        np.save(model_embeddings_dir / 'sample_ids.npy', np.array(sample_ids))\n",
    "        \n",
    "        # Save captions and IDs as structured text file\n",
    "        with open(model_embeddings_dir / 'captions_with_ids.txt', 'w') as f:\n",
    "            f.write(\"# Format: sample_id,caption\\n\")\n",
    "            for sample_id, caption in zip(sample_ids, captions):\n",
    "                # Escape commas in captions\n",
    "                escaped_caption = caption.replace(',', '\\\\,')\n",
    "                f.write(f\"{sample_id},{escaped_caption}\\n\")\n",
    "        \n",
    "        # Save comprehensive information\n",
    "        with open(model_embeddings_dir / 'embedding_info.txt', 'w') as f:\n",
    "            f.write(f\"Model: {model_name}\\n\")\n",
    "            f.write(f\"Total samples: {len(sample_ids)}\\n\")\n",
    "            f.write(f\"Sample ID range: {min(sample_ids)} - {max(sample_ids)}\\n\\n\")\n",
    "            \n",
    "            f.write(f\"UNIFIED EMBEDDINGS:\\n\")\n",
    "            f.write(f\"  Default method: concatenation\\n\")\n",
    "            f.write(f\"  Unified dimension: {unified_embeddings.shape[1]}\\n\")\n",
    "            f.write(f\"  Available fusion methods:\\n\")\n",
    "            f.write(f\"    - Concatenation: {unified_embeddings_concat.shape[1]}D\\n\")\n",
    "            f.write(f\"    - Weighted average: {unified_embeddings_avg.shape[1]}D (text:{text_weight}, image:{image_weight})\\n\")\n",
    "            f.write(f\"    - Element-wise multiply: {unified_embeddings_multiply.shape[1]}D\\n\")\n",
    "            f.write(f\"    - Element-wise max: {unified_embeddings_max.shape[1]}D\\n\\n\")\n",
    "            \n",
    "            f.write(f\"ORIGINAL EMBEDDINGS:\\n\")\n",
    "            f.write(f\"  Text embedding dimension: {text_embeddings.shape[1]}\\n\")\n",
    "            f.write(f\"  Image embedding dimension: {image_embeddings.shape[1]}\\n\\n\")\n",
    "            \n",
    "            f.write(f\"PAIRING RULE:\\n\")\n",
    "            f.write(f\"  unified_embeddings[i] combines info from text_embeddings[i] + image_embeddings[i]\\n\")\n",
    "            f.write(f\"  for captions[i] and sample_ids[i]\\n\\n\")\n",
    "            \n",
    "            f.write(f\"FILES:\\n\")\n",
    "            f.write(f\"  - validation_embeddings.pkl: Complete data with all embedding variants\\n\")\n",
    "            f.write(f\"  - unified_embeddings.npy: Main unified embeddings (concatenation)\\n\")\n",
    "            f.write(f\"  - unified_embeddings_*.npy: Different fusion methods\\n\")\n",
    "            f.write(f\"  - text_embeddings.npy: Original text embeddings\\n\")\n",
    "            f.write(f\"  - image_embeddings.npy: Original image embeddings\\n\")\n",
    "            f.write(f\"  - sample_ids.npy: Sample IDs\\n\")\n",
    "            f.write(f\"  - captions_with_ids.txt: Captions with corresponding IDs\\n\")\n",
    "        \n",
    "        print(f\"   üíæ Saved unified embeddings to: {model_embeddings_dir}\")\n",
    "        print(f\"      Main files: validation_embeddings.pkl, unified_embeddings.npy\")\n",
    "        print(f\"      Fusion variants: unified_embeddings_concat.npy, unified_embeddings_avg.npy,\")\n",
    "        print(f\"                      unified_embeddings_multiply.npy, unified_embeddings_max.npy\")\n",
    "        print(f\"      Original: text_embeddings.npy, image_embeddings.npy\")\n",
    "        print(f\"      Metadata: sample_ids.npy, captions_with_ids.txt, embedding_info.txt\")\n",
    "        \n",
    "        return unified_embeddings, sample_ids, captions, text_embeddings, image_embeddings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error generating unified embeddings for {model_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, None, None\n",
    "\n",
    "print(\"‚úÖ Unified embedding generation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b268b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unified embeddings for all loaded models\n",
    "if val_df is not None and len(loaded_models) > 0:\n",
    "    print(f\"üöÄ Starting unified embedding generation for {len(loaded_models)} models...\")\n",
    "    \n",
    "    embedding_results = {}\n",
    "    \n",
    "    # Process each loaded model\n",
    "    for model_name, model, tokenizer_or_processor in loaded_models:\n",
    "        # Get the appropriate dataset class\n",
    "        if model_name == 'CLIPP-SciBERT':\n",
    "            dataset_class = ImageTextDatasetSciBERT\n",
    "        elif model_name == 'CLIPP-DistilBERT':\n",
    "            dataset_class = ImageTextDatasetDistilBERT\n",
    "        elif model_name == 'MobileCLIP':\n",
    "            dataset_class = ImageTextDatasetMobileCLIP\n",
    "        elif model_name == 'BLIP':\n",
    "            dataset_class = ImageTextDatasetBLIP\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Unknown model type: {model_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Generate unified embeddings\n",
    "        unified_emb, sample_ids, captions, text_emb, img_emb = generate_embeddings_for_model(\n",
    "            model_name, model, tokenizer_or_processor, dataset_class, val_df, device\n",
    "        )\n",
    "        \n",
    "        if unified_emb is not None:\n",
    "            embedding_results[model_name] = {\n",
    "                'unified_embeddings': unified_emb,\n",
    "                'text_embeddings': text_emb,  # Keep for comparison\n",
    "                'image_embeddings': img_emb,  # Keep for comparison\n",
    "                'sample_ids': sample_ids,\n",
    "                'captions': captions\n",
    "            }\n",
    "            \n",
    "            # Verify pairing for this model\n",
    "            print(f\"   üîç Pairing verification for {model_name}:\")\n",
    "            print(f\"      - Unified embedding shape: {unified_emb.shape}\")\n",
    "            print(f\"      - Text/Image embedding shapes: {text_emb.shape}, {img_emb.shape}\")\n",
    "            print(f\"      - Caption count matches: {len(captions) == unified_emb.shape[0]}\")\n",
    "            print(f\"      - Sample ID count matches: {len(sample_ids) == unified_emb.shape[0]}\")\n",
    "            print(f\"      - Sample IDs are unique: {len(set(sample_ids)) == len(sample_ids)}\")\n",
    "            \n",
    "            # Show fusion information\n",
    "            unified_dim = unified_emb.shape[1]\n",
    "            text_dim = text_emb.shape[1]\n",
    "            img_dim = img_emb.shape[1]\n",
    "            print(f\"      - Fusion: {text_dim}D (text) + {img_dim}D (image) ‚Üí {unified_dim}D (unified)\")\n",
    "    \n",
    "    print(f\"\\nüéâ Completed unified embedding generation!\")\n",
    "    print(f\"‚úÖ Successfully generated unified embeddings for {len(embedding_results)} models:\")\n",
    "    for model_name in embedding_results.keys():\n",
    "        print(f\"   - {model_name}\")\n",
    "    \n",
    "    # Show embedding directory structure\n",
    "    print(f\"\\nüìÅ Embeddings directory structure:\")\n",
    "    for item in embeddings_dir.iterdir():\n",
    "        if item.is_dir():\n",
    "            print(f\"   üìÇ {item.name}/\")\n",
    "            for file in sorted(item.iterdir()):\n",
    "                if file.is_file():\n",
    "                    file_size = file.stat().st_size / (1024*1024)  # MB\n",
    "                    print(f\"      üìÑ {file.name} ({file_size:.1f} MB)\")\n",
    "                    \n",
    "    # Show unified embedding statistics\n",
    "    print(f\"\\nüìä Unified Embedding Statistics:\")\n",
    "    for model_name, results in embedding_results.items():\n",
    "        unified = results['unified_embeddings']\n",
    "        print(f\"   {model_name}:\")\n",
    "        print(f\"     Shape: {unified.shape}\")\n",
    "        print(f\"     Mean: {unified.mean():.4f}\")\n",
    "        print(f\"     Std: {unified.std():.4f}\")\n",
    "        print(f\"     Min: {unified.min():.4f}\")\n",
    "        print(f\"     Max: {unified.max():.4f}\")\n",
    "\n",
    "else:\n",
    "    if val_df is None:\n",
    "        print(\"‚ùå Cannot generate embeddings: validation data not loaded\")\n",
    "    if len(loaded_models) == 0:\n",
    "        print(\"‚ùå Cannot generate embeddings: no models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b28851d",
   "metadata": {},
   "source": [
    "### 6.1 Compute Retrieval Metrics on Validation Set\n",
    "\n",
    "Let's compute retrieval metrics (text-to-image and image-to-text) for each model using the generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8629a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unified_similarity_metrics(unified_embeddings, k_values=[1, 5, 10]):\n",
    "    \"\"\"\n",
    "    Compute similarity metrics for unified embeddings (self-similarity analysis).\n",
    "    \n",
    "    Args:\n",
    "        unified_embeddings: tensor of shape (N, D) - unified embeddings\n",
    "        k_values: list of k values for top-k analysis\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with similarity metrics\n",
    "    \"\"\"\n",
    "    # Normalize embeddings for cosine similarity\n",
    "    normalized_emb = unified_embeddings / torch.norm(unified_embeddings, dim=1, keepdims=True)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    similarity_matrix = normalized_emb @ normalized_emb.T  # (N, N)\n",
    "    \n",
    "    # Remove diagonal (self-similarity = 1.0)\n",
    "    mask = torch.eye(similarity_matrix.size(0), dtype=torch.bool)\n",
    "    similarity_matrix_no_diag = similarity_matrix.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Compute statistics\n",
    "    results['similarity_stats'] = {\n",
    "        'mean': similarity_matrix_no_diag[~mask].mean().item(),\n",
    "        'std': similarity_matrix_no_diag[~mask].std().item(),\n",
    "        'min': similarity_matrix_no_diag[~mask].min().item(),\n",
    "        'max': similarity_matrix_no_diag[~mask].max().item()\n",
    "    }\n",
    "    \n",
    "    # Find most similar pairs (excluding self)\n",
    "    results['top_similarities'] = {}\n",
    "    for k in k_values:\n",
    "        top_k_per_sample = torch.topk(similarity_matrix_no_diag, k, dim=1)\n",
    "        avg_top_k = top_k_per_sample.values.mean(dim=1).mean().item()\n",
    "        results['top_similarities'][f'avg_top_{k}'] = avg_top_k\n",
    "    \n",
    "    return results, similarity_matrix\n",
    "\n",
    "def compare_fusion_methods(text_embeddings, image_embeddings):\n",
    "    \"\"\"\n",
    "    Compare different methods of fusing text and image embeddings.\n",
    "    \n",
    "    Args:\n",
    "        text_embeddings: Text embeddings\n",
    "        image_embeddings: Image embeddings\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with different fusion results\n",
    "    \"\"\"\n",
    "    # Ensure same dimensions for fair comparison\n",
    "    min_dim = min(text_embeddings.shape[1], image_embeddings.shape[1])\n",
    "    text_proj = text_embeddings[:, :min_dim]\n",
    "    image_proj = image_embeddings[:, :min_dim]\n",
    "    \n",
    "    fusion_methods = {}\n",
    "    \n",
    "    # Method 1: Concatenation\n",
    "    fusion_methods['concatenation'] = torch.cat([text_embeddings, image_embeddings], dim=1)\n",
    "    \n",
    "    # Method 2: Element-wise average\n",
    "    fusion_methods['average'] = (text_proj + image_proj) / 2\n",
    "    \n",
    "    # Method 3: Weighted average (text heavy)\n",
    "    fusion_methods['text_weighted'] = 0.7 * text_proj + 0.3 * image_proj\n",
    "    \n",
    "    # Method 4: Weighted average (image heavy)  \n",
    "    fusion_methods['image_weighted'] = 0.3 * text_proj + 0.7 * image_proj\n",
    "    \n",
    "    # Method 5: Element-wise multiplication\n",
    "    fusion_methods['multiplication'] = text_proj * image_proj\n",
    "    \n",
    "    # Method 6: Element-wise maximum\n",
    "    fusion_methods['maximum'] = torch.max(text_proj, image_proj)\n",
    "    \n",
    "    # Method 7: Element-wise minimum\n",
    "    fusion_methods['minimum'] = torch.min(text_proj, image_proj)\n",
    "    \n",
    "    # Compute metrics for each method\n",
    "    method_metrics = {}\n",
    "    for method_name, fused_emb in fusion_methods.items():\n",
    "        metrics, _ = compute_unified_similarity_metrics(fused_emb)\n",
    "        method_metrics[method_name] = {\n",
    "            'shape': fused_emb.shape,\n",
    "            'similarity_stats': metrics['similarity_stats'],\n",
    "            'top_similarities': metrics['top_similarities']\n",
    "        }\n",
    "    \n",
    "    return method_metrics\n",
    "\n",
    "# Compute metrics for unified embeddings\n",
    "if embedding_results:\n",
    "    print(\"üìä Computing metrics for unified embeddings:\\n\")\n",
    "    \n",
    "    unified_metrics = {}\n",
    "    \n",
    "    for model_name, embeddings in embedding_results.items():\n",
    "        print(f\"üîç {model_name}:\")\n",
    "        \n",
    "        unified_emb = embeddings['unified_embeddings']\n",
    "        text_emb = embeddings['text_embeddings']  \n",
    "        img_emb = embeddings['image_embeddings']\n",
    "        \n",
    "        # Compute unified embedding metrics\n",
    "        metrics, similarity_matrix = compute_unified_similarity_metrics(unified_emb)\n",
    "        \n",
    "        print(f\"   üìà Unified Embedding Similarity Stats:\")\n",
    "        stats = metrics['similarity_stats']\n",
    "        print(f\"      Mean similarity: {stats['mean']:.4f}\")\n",
    "        print(f\"      Std similarity:  {stats['std']:.4f}\")\n",
    "        print(f\"      Min similarity:  {stats['min']:.4f}\")\n",
    "        print(f\"      Max similarity:  {stats['max']:.4f}\")\n",
    "        \n",
    "        print(f\"   üèÜ Top-K Average Similarities:\")\n",
    "        for k, sim in metrics['top_similarities'].items():\n",
    "            print(f\"      {k.replace('_', '-').title()}: {sim:.4f}\")\n",
    "        \n",
    "        # Compare fusion methods\n",
    "        print(f\"   \udd2c Comparing fusion methods:\")\n",
    "        fusion_comparison = compare_fusion_methods(text_emb, img_emb)\n",
    "        \n",
    "        for method, method_metrics in fusion_comparison.items():\n",
    "            shape = method_metrics['shape']\n",
    "            mean_sim = method_metrics['similarity_stats']['mean']\n",
    "            print(f\"      {method:<15}: {shape} ‚Üí mean_sim: {mean_sim:.4f}\")\n",
    "        \n",
    "        unified_metrics[model_name] = {\n",
    "            'unified_metrics': metrics,\n",
    "            'fusion_comparison': fusion_comparison\n",
    "        }\n",
    "        print()\n",
    "    \n",
    "    # Create unified embedding comparison table\n",
    "    print(\"üìã UNIFIED EMBEDDING COMPARISON TABLE:\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"{'Model':<20} {'Dimension':<12} {'Mean Sim':<10} {'Std Sim':<10} {'Max Sim':<10} {'Top-1 Avg':<10} {'Top-5 Avg':<10}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    for model_name, metrics in unified_metrics.items():\n",
    "        unified_shape = embedding_results[model_name]['unified_embeddings'].shape[1]\n",
    "        stats = metrics['unified_metrics']['similarity_stats']\n",
    "        top_sims = metrics['unified_metrics']['top_similarities']\n",
    "        \n",
    "        print(f\"{model_name:<20} {unified_shape:<12} {stats['mean']:<10.4f} {stats['std']:<10.4f} \"\n",
    "              f\"{stats['max']:<10.4f} {top_sims['avg_top_1']:<10.4f} {top_sims['avg_top_5']:<10.4f}\")\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Save unified metrics to file\n",
    "    metrics_file = embeddings_dir / 'unified_embedding_metrics.pkl'\n",
    "    with open(metrics_file, 'wb') as f:\n",
    "        pickle.dump(unified_metrics, f)\n",
    "    print(f\"üíæ Saved unified embedding metrics to: {metrics_file}\")\n",
    "    \n",
    "    # Also save fusion comparison\n",
    "    fusion_file = embeddings_dir / 'fusion_method_comparison.pkl'\n",
    "    fusion_comparison_all = {model: metrics['fusion_comparison'] for model, metrics in unified_metrics.items()}\n",
    "    with open(fusion_file, 'wb') as f:\n",
    "        pickle.dump(fusion_comparison_all, f)\n",
    "    print(f\"üíæ Saved fusion method comparison to: {fusion_file}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No unified embeddings available for computing metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eda5d8",
   "metadata": {},
   "source": [
    "### 6.2 Load Embeddings from Saved Files\n",
    "\n",
    "You can also load the embeddings later from the saved files for analysis or further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69bdaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_embeddings(model_name, embeddings_dir='./embeddings', embedding_type='unified'):\n",
    "    \"\"\"\n",
    "    Load previously saved embeddings for a model with focus on unified embeddings.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        embeddings_dir: Directory containing saved embeddings\n",
    "        embedding_type: Type of embedding to prioritize ('unified', 'text', 'image', 'all')\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with loaded embeddings, IDs, and captions\n",
    "    \"\"\"\n",
    "    embeddings_path = Path(embeddings_dir)\n",
    "    model_dir = embeddings_path / model_name.lower().replace('-', '_')\n",
    "    \n",
    "    if not model_dir.exists():\n",
    "        print(f\"‚ùå No embeddings found for {model_name} at {model_dir}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load pickle file if available (preferred - contains all data)\n",
    "        pickle_file = model_dir / 'validation_embeddings.pkl'\n",
    "        if pickle_file.exists():\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            print(f\"‚úÖ Loaded embeddings for {model_name} from pickle file\")\n",
    "            \n",
    "            # Verify unified embedding information\n",
    "            if 'fusion_info' in data:\n",
    "                fusion = data['fusion_info']\n",
    "                print(f\"   \udd17 Fusion method: {fusion['default_method']}\")\n",
    "                print(f\"   üìè Unified dim: {fusion['unified_dim']}, Text: {fusion['original_text_dim']}, Image: {fusion['original_image_dim']}\")\n",
    "            \n",
    "            # Verify data consistency\n",
    "            unified_emb = data.get('unified_embeddings')\n",
    "            text_emb = data.get('text_embeddings')\n",
    "            img_emb = data.get('image_embeddings')\n",
    "            captions = data['captions']\n",
    "            sample_ids = data.get('sample_ids', list(range(len(captions))))\n",
    "            \n",
    "            if unified_emb is not None:\n",
    "                assert len(captions) == len(sample_ids) == unified_emb.shape[0], \\\n",
    "                    f\"Data inconsistency: captions={len(captions)}, ids={len(sample_ids)}, unified_emb={unified_emb.shape[0]}\"\n",
    "                print(f\"   ‚úÖ Unified embedding pairing verified: {len(sample_ids)} consistent pairs\")\n",
    "            \n",
    "            return data\n",
    "        \n",
    "        # Otherwise load from separate numpy files\n",
    "        unified_emb_file = model_dir / 'unified_embeddings.npy'\n",
    "        text_emb_file = model_dir / 'text_embeddings.npy'\n",
    "        img_emb_file = model_dir / 'image_embeddings.npy'\n",
    "        sample_ids_file = model_dir / 'sample_ids.npy'\n",
    "        captions_file = model_dir / 'captions_with_ids.txt'\n",
    "        \n",
    "        # Check what's available\n",
    "        available_files = [f for f in [unified_emb_file, text_emb_file, img_emb_file, sample_ids_file, captions_file] if f.exists()]\n",
    "        \n",
    "        if len(available_files) >= 3:  # At least embeddings, ids, and captions\n",
    "            data = {'model_name': model_name}\n",
    "            \n",
    "            # Load unified embeddings if available\n",
    "            if unified_emb_file.exists():\n",
    "                data['unified_embeddings'] = np.load(unified_emb_file)\n",
    "                print(f\"‚úÖ Loaded unified embeddings: {data['unified_embeddings'].shape}\")\n",
    "                \n",
    "                # Load other fusion variants if available\n",
    "                for variant in ['concat', 'avg', 'multiply', 'max']:\n",
    "                    variant_file = model_dir / f'unified_embeddings_{variant}.npy'\n",
    "                    if variant_file.exists():\n",
    "                        data[f'unified_embeddings_{variant}'] = np.load(variant_file)\n",
    "            \n",
    "            # Load original embeddings if available\n",
    "            if text_emb_file.exists():\n",
    "                data['text_embeddings'] = np.load(text_emb_file)\n",
    "            if img_emb_file.exists():\n",
    "                data['image_embeddings'] = np.load(img_emb_file)\n",
    "            \n",
    "            # Load metadata\n",
    "            if sample_ids_file.exists():\n",
    "                data['sample_ids'] = np.load(sample_ids_file).tolist()\n",
    "            \n",
    "            if captions_file.exists():\n",
    "                captions = []\n",
    "                with open(captions_file, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                    for line in lines:\n",
    "                        if line.startswith('#') or not line.strip():\n",
    "                            continue\n",
    "                        parts = line.strip().split(',', 1)\n",
    "                        if len(parts) == 2:\n",
    "                            caption = parts[1].replace('\\\\,', ',')  # Unescape commas\n",
    "                            captions.append(caption)\n",
    "                data['captions'] = captions\n",
    "            \n",
    "            # Add metadata\n",
    "            if 'unified_embeddings' in data:\n",
    "                data['fusion_info'] = {\n",
    "                    'default_method': 'concatenation',\n",
    "                    'unified_dim': data['unified_embeddings'].shape[1]\n",
    "                }\n",
    "            \n",
    "            print(f\"‚úÖ Loaded embeddings for {model_name} from separate files\")\n",
    "            print(f\"   ‚úÖ Available: {list(data.keys())}\")\n",
    "            return data\n",
    "        \n",
    "        else:\n",
    "            print(f\"‚ùå Insufficient embedding files for {model_name}\")\n",
    "            missing_files = [f.name for f in [unified_emb_file, sample_ids_file, captions_file] if not f.exists()]\n",
    "            print(f\"   Missing critical files: {missing_files}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading embeddings for {model_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def verify_unified_embedding_pairing(data):\n",
    "    \"\"\"\n",
    "    Verify that unified embeddings are properly constructed and paired.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary with embeddings data\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if pairing is correct\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        unified_emb = data.get('unified_embeddings')\n",
    "        text_emb = data.get('text_embeddings')\n",
    "        img_emb = data.get('image_embeddings') \n",
    "        captions = data.get('captions', [])\n",
    "        sample_ids = data.get('sample_ids', list(range(len(captions))))\n",
    "        \n",
    "        print(f\"üîç Unified embedding verification for {data.get('model_name', 'Unknown')}:\")\n",
    "        \n",
    "        if unified_emb is not None:\n",
    "            print(f\"   Unified embeddings: {unified_emb.shape}\")\n",
    "        if text_emb is not None:\n",
    "            print(f\"   Text embeddings: {text_emb.shape}\")\n",
    "        if img_emb is not None:\n",
    "            print(f\"   Image embeddings: {img_emb.shape}\")\n",
    "        print(f\"   Captions: {len(captions)}\")\n",
    "        print(f\"   Sample IDs: {len(sample_ids)}\")\n",
    "        \n",
    "        # Check unified embedding consistency\n",
    "        if unified_emb is not None:\n",
    "            unified_consistent = (unified_emb.shape[0] == len(captions) == len(sample_ids))\n",
    "            print(f\"   ‚úÖ Unified embedding consistency: {unified_consistent}\")\n",
    "            \n",
    "            # Check if unified dimension makes sense\n",
    "            if text_emb is not None and img_emb is not None:\n",
    "                expected_concat_dim = text_emb.shape[1] + img_emb.shape[1]\n",
    "                is_concatenated = (unified_emb.shape[1] == expected_concat_dim)\n",
    "                print(f\"   üîó Appears to be concatenated: {is_concatenated} ({unified_emb.shape[1]} vs {expected_concat_dim})\")\n",
    "            \n",
    "            # Show fusion information if available\n",
    "            if 'fusion_info' in data:\n",
    "                fusion = data['fusion_info']\n",
    "                print(f\"   üìã Fusion method: {fusion.get('default_method', 'unknown')}\")\n",
    "                print(f\"   üìè Dimensions: {fusion.get('unified_dim', 'unknown')}D unified\")\n",
    "        \n",
    "        # Check ID uniqueness\n",
    "        ids_unique = len(set(sample_ids)) == len(sample_ids)\n",
    "        print(f\"   ‚úÖ Sample IDs unique: {ids_unique}\")\n",
    "        \n",
    "        # Sample some unified embeddings\n",
    "        if unified_emb is not None and len(captions) > 0:\n",
    "            print(f\"   üìã Sample unified embeddings:\")\n",
    "            for i in [0, len(captions)//2, -1]:\n",
    "                if i < len(captions):\n",
    "                    idx = i if i >= 0 else len(captions) + i\n",
    "                    emb_norm = np.linalg.norm(unified_emb[idx])\n",
    "                    print(f\"      [{idx}] ID:{sample_ids[idx]} norm:{emb_norm:.4f} -> \\\"{captions[idx][:40]}...\\\"\")\n",
    "        \n",
    "        return unified_emb is not None and unified_consistent and ids_unique\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error during verification: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example: Load unified embeddings from saved files\n",
    "print(\"üìÇ Available embedding directories:\")\n",
    "if embeddings_dir.exists():\n",
    "    for item in embeddings_dir.iterdir():\n",
    "        if item.is_dir():\n",
    "            print(f\"   üìÅ {item.name}\")\n",
    "            \n",
    "    # Try to load an example (if any exist)\n",
    "    model_dirs = [d for d in embeddings_dir.iterdir() if d.is_dir()]\n",
    "    if model_dirs:\n",
    "        example_model = model_dirs[0].name.replace('_', '-').upper()\n",
    "        print(f\"\\nüîç Example: Loading unified embeddings for {example_model}\")\n",
    "        loaded_data = load_saved_embeddings(example_model, embeddings_dir, embedding_type='unified')\n",
    "        \n",
    "        if loaded_data:\n",
    "            verify_unified_embedding_pairing(loaded_data)\n",
    "            \n",
    "            # Show available fusion methods\n",
    "            if 'unified_embeddings_concat' in loaded_data:\n",
    "                print(f\"\\n   üîó Available fusion variants:\")\n",
    "                for key in loaded_data.keys():\n",
    "                    if key.startswith('unified_embeddings_') and isinstance(loaded_data[key], np.ndarray):\n",
    "                        variant = key.replace('unified_embeddings_', '')\n",
    "                        shape = loaded_data[key].shape\n",
    "                        print(f\"      {variant}: {shape}\")\n",
    "    else:\n",
    "        print(\"   (No model directories found)\")\n",
    "else:\n",
    "    print(\"   (Embeddings directory does not exist yet)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8858775f",
   "metadata": {},
   "source": [
    "### 6.3 Demonstrate Proper Text-Image Pairing\n",
    "\n",
    "Let's show how to properly match text and image embeddings using the sample IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f90a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_pairing(embedding_data, num_examples=5):\n",
    "    \"\"\"\n",
    "    Demonstrate how text and image embeddings are properly paired using sample IDs.\n",
    "    \n",
    "    Args:\n",
    "        embedding_data: Dictionary with embeddings and metadata\n",
    "        num_examples: Number of examples to show\n",
    "    \"\"\"\n",
    "    if not embedding_data:\n",
    "        print(\"‚ùå No embedding data provided\")\n",
    "        return\n",
    "    \n",
    "    text_emb = embedding_data['text_embeddings']\n",
    "    img_emb = embedding_data['image_embeddings']\n",
    "    captions = embedding_data['captions']\n",
    "    sample_ids = embedding_data.get('sample_ids', list(range(len(captions))))\n",
    "    model_name = embedding_data.get('model_name', 'Unknown')\n",
    "    \n",
    "    print(f\"üîó Demonstrating pairing for {model_name}:\")\n",
    "    print(f\"   Total pairs: {len(sample_ids)}\")\n",
    "    print(f\"   Text embedding dim: {text_emb.shape[1]}\")\n",
    "    print(f\"   Image embedding dim: {img_emb.shape[1]}\")\n",
    "    print()\n",
    "    \n",
    "    # Show some examples\n",
    "    indices = np.linspace(0, len(sample_ids)-1, num_examples, dtype=int)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        sample_id = sample_ids[idx]\n",
    "        caption = captions[idx]\n",
    "        text_embedding = text_emb[idx]\n",
    "        image_embedding = img_emb[idx]\n",
    "        \n",
    "        print(f\"üìù Example {i+1} (Index {idx}):\")\n",
    "        print(f\"   Sample ID: {sample_id}\")\n",
    "        print(f\"   Caption: \\\"{caption[:80]}{'...' if len(caption) > 80 else ''}\\\"\")\n",
    "        print(f\"   Text embedding: shape={text_embedding.shape}, norm={np.linalg.norm(text_embedding):.4f}\")\n",
    "        print(f\"   Image embedding: shape={image_embedding.shape}, norm={np.linalg.norm(image_embedding):.4f}\")\n",
    "        \n",
    "        # Compute similarity between paired text and image\n",
    "        similarity = np.dot(text_embedding, image_embedding) / (np.linalg.norm(text_embedding) * np.linalg.norm(image_embedding))\n",
    "        print(f\"   Text-Image similarity: {similarity:.4f}\")\n",
    "        print()\n",
    "\n",
    "def find_most_similar_pairs(embedding_data, top_k=5):\n",
    "    \"\"\"\n",
    "    Find the most similar text-image pairs (should be the diagonal if properly paired).\n",
    "    \n",
    "    Args:\n",
    "        embedding_data: Dictionary with embeddings and metadata\n",
    "        top_k: Number of top similar pairs to show\n",
    "    \"\"\"\n",
    "    if not embedding_data:\n",
    "        print(\"‚ùå No embedding data provided\")\n",
    "        return\n",
    "    \n",
    "    text_emb = embedding_data['text_embeddings']\n",
    "    img_emb = embedding_data['image_embeddings']\n",
    "    captions = embedding_data['captions']\n",
    "    sample_ids = embedding_data.get('sample_ids', list(range(len(captions))))\n",
    "    model_name = embedding_data.get('model_name', 'Unknown')\n",
    "    \n",
    "    print(f\"üîç Finding most similar text-image pairs for {model_name}:\")\n",
    "    \n",
    "    # Normalize embeddings for cosine similarity\n",
    "    text_emb_norm = text_emb / np.linalg.norm(text_emb, axis=1, keepdims=True)\n",
    "    img_emb_norm = img_emb / np.linalg.norm(img_emb, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    similarity_matrix = text_emb_norm @ img_emb_norm.T\n",
    "    \n",
    "    # Get diagonal similarities (correct pairs)\n",
    "    diagonal_similarities = np.diag(similarity_matrix)\n",
    "    \n",
    "    # Find top-k most similar pairs overall\n",
    "    flat_similarities = similarity_matrix.flatten()\n",
    "    top_indices = np.argsort(flat_similarities)[-top_k:][::-1]\n",
    "    \n",
    "    print(f\"   üìä Diagonal similarity stats (correct pairs):\")\n",
    "    print(f\"      Mean: {diagonal_similarities.mean():.4f}\")\n",
    "    print(f\"      Std:  {diagonal_similarities.std():.4f}\")\n",
    "    print(f\"      Min:  {diagonal_similarities.min():.4f}\")\n",
    "    print(f\"      Max:  {diagonal_similarities.max():.4f}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"   üèÜ Top {top_k} most similar pairs overall:\")\n",
    "    for rank, flat_idx in enumerate(top_indices):\n",
    "        text_idx = flat_idx // similarity_matrix.shape[1]\n",
    "        img_idx = flat_idx % similarity_matrix.shape[1]\n",
    "        similarity = similarity_matrix[text_idx, img_idx]\n",
    "        \n",
    "        is_correct_pair = (text_idx == img_idx)\n",
    "        pair_type = \"‚úÖ CORRECT\" if is_correct_pair else \"‚ùå INCORRECT\"\n",
    "        \n",
    "        print(f\"      {rank+1}. Text[{text_idx}] <-> Image[{img_idx}]: {similarity:.4f} {pair_type}\")\n",
    "        if not is_correct_pair:\n",
    "            print(f\"         Text: \\\"{captions[text_idx][:60]}...\\\"\")\n",
    "        \n",
    "    # Check if top pairs are mostly diagonal (good sign)\n",
    "    correct_in_top = sum(1 for flat_idx in top_indices if (flat_idx // similarity_matrix.shape[1]) == (flat_idx % similarity_matrix.shape[1]))\n",
    "    print(f\"\\n   üìà {correct_in_top}/{top_k} top pairs are correctly paired\")\n",
    "\n",
    "# Test pairing demonstration if we have embedding results\n",
    "if 'embedding_results' in locals() and embedding_results:\n",
    "    print(\"üß™ Testing pairing demonstration with generated embeddings:\\n\")\n",
    "    \n",
    "    # Pick the first available model\n",
    "    model_name = list(embedding_results.keys())[0]\n",
    "    demo_data = {\n",
    "        'text_embeddings': embedding_results[model_name]['text_embeddings'].numpy(),\n",
    "        'image_embeddings': embedding_results[model_name]['image_embeddings'].numpy(),\n",
    "        'captions': embedding_results[model_name]['captions'],\n",
    "        'sample_ids': embedding_results[model_name]['sample_ids'],\n",
    "        'model_name': model_name\n",
    "    }\n",
    "    \n",
    "    demonstrate_pairing(demo_data, num_examples=3)\n",
    "    find_most_similar_pairs(demo_data, top_k=10)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No embedding results available for pairing demonstration\")\n",
    "    print(\"   Run the embedding generation cells first to see pairing examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4885d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Execute the complete unified embedding generation pipeline\n",
    "print(\"=\" * 80)\n",
    "print(\"üöÄ STARTING UNIFIED EMBEDDING GENERATION PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Generate unified embeddings for all models\n",
    "    print(f\"\\nüìä Processing {len(val_df)} validation samples...\")\n",
    "    print(f\"üìÅ Saving to: {embeddings_dir}\")\n",
    "    \n",
    "    # Track overall statistics\n",
    "    total_models = len(MODELS_TO_TEST)\n",
    "    successful_models = 0\n",
    "    failed_models = []\n",
    "    fusion_stats = {}\n",
    "    \n",
    "    for i, model_name in enumerate(MODELS_TO_TEST, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üîÑ [{i}/{total_models}] Processing {model_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            # Generate unified embeddings\n",
    "            embedding_data = generate_embeddings_for_model(model_name, val_df, embeddings_dir)\n",
    "            \n",
    "            if embedding_data and 'unified_embeddings' in embedding_data:\n",
    "                successful_models += 1\n",
    "                \n",
    "                # Collect fusion statistics\n",
    "                if 'fusion_info' in embedding_data:\n",
    "                    fusion = embedding_data['fusion_info']\n",
    "                    model_key = model_name.lower().replace('-', '_')\n",
    "                    fusion_stats[model_key] = {\n",
    "                        'method': fusion['default_method'],\n",
    "                        'unified_dim': fusion['unified_dim'],\n",
    "                        'text_dim': fusion.get('original_text_dim', 'unknown'),\n",
    "                        'image_dim': fusion.get('original_image_dim', 'unknown'),\n",
    "                        'compression_ratio': fusion.get('compression_ratio', 'unknown')\n",
    "                    }\n",
    "                \n",
    "                print(f\"‚úÖ Successfully generated unified embeddings for {model_name}\")\n",
    "                \n",
    "                # Quick verification\n",
    "                verify_unified_embedding_pairing(embedding_data)\n",
    "                \n",
    "            else:\n",
    "                failed_models.append(model_name)\n",
    "                print(f\"‚ùå Failed to generate unified embeddings for {model_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed_models.append(model_name)\n",
    "            print(f\"‚ùå Exception processing {model_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üéØ UNIFIED EMBEDDING GENERATION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"‚úÖ Successful models: {successful_models}/{total_models}\")\n",
    "    print(f\"‚ùå Failed models: {len(failed_models)}\")\n",
    "    \n",
    "    if failed_models:\n",
    "        print(f\"   Failed: {', '.join(failed_models)}\")\n",
    "    \n",
    "    if fusion_stats:\n",
    "        print(f\"\\nüìä FUSION STATISTICS:\")\n",
    "        for model, stats in fusion_stats.items():\n",
    "            print(f\"   üîó {model.upper()}:\")\n",
    "            print(f\"      Method: {stats['method']}\")\n",
    "            print(f\"      Dimensions: {stats['text_dim']}D text + {stats['image_dim']}D image ‚Üí {stats['unified_dim']}D unified\")\n",
    "            if stats['compression_ratio'] != 'unknown':\n",
    "                print(f\"      Compression: {stats['compression_ratio']:.2f}x\")\n",
    "    \n",
    "    # Show embedding directory structure\n",
    "    print(f\"\\nüìÅ SAVED EMBEDDING STRUCTURE:\")\n",
    "    if embeddings_dir.exists():\n",
    "        for model_dir in sorted(embeddings_dir.iterdir()):\n",
    "            if model_dir.is_dir():\n",
    "                print(f\"   üìÇ {model_dir.name}/\")\n",
    "                for file in sorted(model_dir.iterdir()):\n",
    "                    if file.is_file():\n",
    "                        size_kb = file.stat().st_size / 1024\n",
    "                        print(f\"      üìÑ {file.name} ({size_kb:.1f} KB)\")\n",
    "    \n",
    "    print(f\"\\nüéâ Unified embedding generation completed!\")\n",
    "    print(f\"üìÇ All embeddings saved to: {embeddings_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nüí• Pipeline failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef13af7d",
   "metadata": {},
   "source": [
    "# üìù Text Embedding Pipeline for Individual Models\n",
    "\n",
    "This section provides a pipeline to generate text embeddings for arbitrary text inputs using each individual model. Perfect for testing and exploring model capabilities with custom text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d13413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_embedding_pipeline(model_name):\n",
    "    \"\"\"\n",
    "    Create a text embedding pipeline for a specific model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model ('CLIPP-SciBERT', 'CLIPP-DistilBERT', 'MobileCLIP', 'BLIP')\n",
    "    \n",
    "    Returns:\n",
    "        function: A pipeline function that takes text and returns embeddings\n",
    "    \"\"\"\n",
    "    print(f\"üîß Creating text embedding pipeline for {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the model\n",
    "        if model_name == 'CLIPP-SciBERT':\n",
    "            model, tokenizer, processor, device = load_clipp_scibert_model()\n",
    "            \n",
    "            def pipeline(text_input):\n",
    "                \"\"\"Generate text embedding using CLIPP-SciBERT.\"\"\"\n",
    "                if isinstance(text_input, list):\n",
    "                    texts = text_input\n",
    "                else:\n",
    "                    texts = [text_input]\n",
    "                \n",
    "                model.eval()\n",
    "                embeddings = []\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for text in texts:\n",
    "                        # Tokenize text\n",
    "                        text_tokens = tokenizer(text, padding=True, truncation=True, \n",
    "                                              return_tensors=\"pt\", max_length=512).to(device)\n",
    "                        \n",
    "                        # Get text embedding\n",
    "                        text_features = model.encode_text(text_tokens['input_ids'], text_tokens['attention_mask'])\n",
    "                        text_features = F.normalize(text_features, p=2, dim=1)\n",
    "                        \n",
    "                        embeddings.append(text_features.cpu().numpy())\n",
    "                \n",
    "                result = np.vstack(embeddings) if len(embeddings) > 1 else embeddings[0]\n",
    "                return result.squeeze() if len(texts) == 1 else result\n",
    "                \n",
    "        elif model_name == 'CLIPP-DistilBERT':\n",
    "            model, tokenizer, processor, device = load_clipp_distilbert_model()\n",
    "            \n",
    "            def pipeline(text_input):\n",
    "                \"\"\"Generate text embedding using CLIPP-DistilBERT.\"\"\"\n",
    "                if isinstance(text_input, list):\n",
    "                    texts = text_input\n",
    "                else:\n",
    "                    texts = [text_input]\n",
    "                \n",
    "                model.eval()\n",
    "                embeddings = []\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for text in texts:\n",
    "                        # Tokenize text\n",
    "                        text_tokens = tokenizer(text, padding=True, truncation=True, \n",
    "                                              return_tensors=\"pt\", max_length=512).to(device)\n",
    "                        \n",
    "                        # Get text embedding\n",
    "                        text_features = model.encode_text(text_tokens['input_ids'], text_tokens['attention_mask'])\n",
    "                        text_features = F.normalize(text_features, p=2, dim=1)\n",
    "                        \n",
    "                        embeddings.append(text_features.cpu().numpy())\n",
    "                \n",
    "                result = np.vstack(embeddings) if len(embeddings) > 1 else embeddings[0]\n",
    "                return result.squeeze() if len(texts) == 1 else result\n",
    "                \n",
    "        elif model_name == 'MobileCLIP':\n",
    "            model, tokenizer, processor, device = load_apple_mobileclip_model()\n",
    "            \n",
    "            def pipeline(text_input):\n",
    "                \"\"\"Generate text embedding using MobileCLIP.\"\"\"\n",
    "                if isinstance(text_input, list):\n",
    "                    texts = text_input\n",
    "                else:\n",
    "                    texts = [text_input]\n",
    "                \n",
    "                model.eval()\n",
    "                embeddings = []\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for text in texts:\n",
    "                        # Tokenize text using open_clip tokenizer\n",
    "                        text_tokens = open_clip.tokenize([text]).to(device)\n",
    "                        \n",
    "                        # Get text embedding\n",
    "                        text_features = model.encode_text(text_tokens)\n",
    "                        text_features = F.normalize(text_features, p=2, dim=1)\n",
    "                        \n",
    "                        embeddings.append(text_features.cpu().numpy())\n",
    "                \n",
    "                result = np.vstack(embeddings) if len(embeddings) > 1 else embeddings[0]\n",
    "                return result.squeeze() if len(texts) == 1 else result\n",
    "                \n",
    "        elif model_name == 'BLIP':\n",
    "            model, processor, device = load_blip_model()\n",
    "            \n",
    "            def pipeline(text_input):\n",
    "                \"\"\"Generate text embedding using BLIP.\"\"\"\n",
    "                if isinstance(text_input, list):\n",
    "                    texts = text_input\n",
    "                else:\n",
    "                    texts = [text_input]\n",
    "                \n",
    "                model.eval()\n",
    "                embeddings = []\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for text in texts:\n",
    "                        # Process text\n",
    "                        inputs = processor(text=[text], return_tensors=\"pt\", \n",
    "                                         padding=True, truncation=True, max_length=512).to(device)\n",
    "                        \n",
    "                        # Get text embedding\n",
    "                        text_embeds = model.get_text_features(**inputs)\n",
    "                        text_embeds = F.normalize(text_embeds, p=2, dim=1)\n",
    "                        \n",
    "                        embeddings.append(text_embeds.cpu().numpy())\n",
    "                \n",
    "                result = np.vstack(embeddings) if len(embeddings) > 1 else embeddings[0]\n",
    "                return result.squeeze() if len(texts) == 1 else result\n",
    "                \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model: {model_name}\")\n",
    "            \n",
    "        print(f\"‚úÖ Text embedding pipeline created for {model_name}\")\n",
    "        return pipeline\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create pipeline for {model_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def create_all_text_pipelines():\n",
    "    \"\"\"\n",
    "    Create text embedding pipelines for all available models.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary mapping model names to their pipeline functions\n",
    "    \"\"\"\n",
    "    print(\"üè≠ Creating text embedding pipelines for all models...\")\n",
    "    pipelines = {}\n",
    "    \n",
    "    for model_name in MODELS_TO_TEST:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        pipeline = create_text_embedding_pipeline(model_name)\n",
    "        if pipeline:\n",
    "            pipelines[model_name] = pipeline\n",
    "            print(f\"‚úÖ Pipeline ready for {model_name}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to create pipeline for {model_name}\")\n",
    "    \n",
    "    print(f\"\\nüéâ Created {len(pipelines)}/{len(MODELS_TO_TEST)} text embedding pipelines\")\n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a804c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_text_embeddings_with_examples():\n",
    "    \"\"\"\n",
    "    Test text embedding pipelines with example material science texts.\n",
    "    \"\"\"\n",
    "    print(\"üß™ Testing text embedding pipelines with example materials...\")\n",
    "    \n",
    "    # Example material science texts\n",
    "    example_texts = [\n",
    "        \"Silicon dioxide thin film with high dielectric constant\",\n",
    "        \"Graphene-based composite material for energy storage applications\",\n",
    "        \"Perovskite solar cell with enhanced efficiency and stability\",\n",
    "        \"Titanium alloy with superior mechanical properties\",\n",
    "        \"Carbon nanotube reinforced polymer matrix composite\",\n",
    "        \"Aluminum oxide nanoparticles for catalytic applications\",\n",
    "        \"Copper-zinc alloy with antimicrobial properties\",\n",
    "        \"Lithium-ion battery cathode material with high capacity\"\n",
    "    ]\n",
    "    \n",
    "    # Create all pipelines\n",
    "    pipelines = create_all_text_pipelines()\n",
    "    \n",
    "    if not pipelines:\n",
    "        print(\"‚ùå No pipelines available for testing\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüî¨ Testing with {len(example_texts)} example texts...\")\n",
    "    print(f\"üìã Available pipelines: {list(pipelines.keys())}\")\n",
    "    \n",
    "    # Test each pipeline\n",
    "    results = {}\n",
    "    for model_name, pipeline in pipelines.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üß™ Testing {model_name} pipeline\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            # Test single text\n",
    "            single_text = example_texts[0]\n",
    "            print(f\"üìù Single text: \\\"{single_text[:50]}...\\\"\")\n",
    "            \n",
    "            single_embedding = pipeline(single_text)\n",
    "            print(f\"‚úÖ Single embedding shape: {single_embedding.shape}\")\n",
    "            print(f\"   Embedding norm: {np.linalg.norm(single_embedding):.4f}\")\n",
    "            print(f\"   Sample values: [{single_embedding[0]:.4f}, {single_embedding[1]:.4f}, ..., {single_embedding[-1]:.4f}]\")\n",
    "            \n",
    "            # Test batch processing\n",
    "            batch_texts = example_texts[:3]\n",
    "            print(f\"\\nüìù Batch processing {len(batch_texts)} texts...\")\n",
    "            \n",
    "            batch_embeddings = pipeline(batch_texts)\n",
    "            print(f\"‚úÖ Batch embeddings shape: {batch_embeddings.shape}\")\n",
    "            print(f\"   Individual norms: {[f'{np.linalg.norm(emb):.4f}' for emb in batch_embeddings]}\")\n",
    "            \n",
    "            # Compute similarities within batch\n",
    "            if len(batch_embeddings.shape) > 1 and batch_embeddings.shape[0] > 1:\n",
    "                similarities = np.dot(batch_embeddings, batch_embeddings.T)\n",
    "                print(f\"   Similarity matrix diagonal: {np.diag(similarities)}\")\n",
    "                print(f\"   Off-diagonal similarities: {similarities[0,1]:.4f}, {similarities[0,2]:.4f}, {similarities[1,2]:.4f}\")\n",
    "            \n",
    "            # Store results\n",
    "            results[model_name] = {\n",
    "                'single_embedding': single_embedding,\n",
    "                'batch_embeddings': batch_embeddings,\n",
    "                'embedding_dim': single_embedding.shape[-1],\n",
    "                'success': True\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ {model_name} pipeline test successful\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {model_name} pipeline test failed: {e}\")\n",
    "            results[model_name] = {'success': False, 'error': str(e)}\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üéØ TEXT EMBEDDING PIPELINE TEST SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    successful_models = [name for name, result in results.items() if result.get('success', False)]\n",
    "    failed_models = [name for name, result in results.items() if not result.get('success', False)]\n",
    "    \n",
    "    print(f\"‚úÖ Successful models: {len(successful_models)}/{len(results)}\")\n",
    "    print(f\"‚ùå Failed models: {len(failed_models)}\")\n",
    "    \n",
    "    if successful_models:\n",
    "        print(f\"\\nüìä EMBEDDING DIMENSIONS:\")\n",
    "        for model_name in successful_models:\n",
    "            dim = results[model_name]['embedding_dim']\n",
    "            print(f\"   üîó {model_name}: {dim}D\")\n",
    "    \n",
    "    if failed_models:\n",
    "        print(f\"\\n‚ùå Failed models: {', '.join(failed_models)}\")\n",
    "        for model_name in failed_models:\n",
    "            error = results[model_name].get('error', 'Unknown error')\n",
    "            print(f\"   {model_name}: {error}\")\n",
    "    \n",
    "    print(f\"\\nüéâ Text embedding pipeline testing completed!\")\n",
    "    return results\n",
    "\n",
    "def interactive_text_embedding_demo(pipelines=None):\n",
    "    \"\"\"\n",
    "    Interactive demo for testing text embeddings with custom input.\n",
    "    \n",
    "    Args:\n",
    "        pipelines: Dict of model pipelines (will create if None)\n",
    "    \"\"\"\n",
    "    if pipelines is None:\n",
    "        print(\"üîß Creating pipelines for interactive demo...\")\n",
    "        pipelines = create_all_text_pipelines()\n",
    "    \n",
    "    if not pipelines:\n",
    "        print(\"‚ùå No pipelines available for demo\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüéÆ INTERACTIVE TEXT EMBEDDING DEMO\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Available models: {', '.join(pipelines.keys())}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Demo texts (can be customized)\n",
    "    demo_texts = [\n",
    "        \"High-performance lithium-ion battery material\",\n",
    "        \"Transparent conducting oxide thin film\",\n",
    "        \"Magnetic nanoparticles for biomedical applications\",\n",
    "        \"Flexible organic photovoltaic device\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"üìù Demo texts:\")\n",
    "    for i, text in enumerate(demo_texts, 1):\n",
    "        print(f\"   {i}. {text}\")\n",
    "    \n",
    "    # Generate embeddings for all models\n",
    "    print(f\"\\nüîÑ Generating embeddings for all models...\")\n",
    "    \n",
    "    all_embeddings = {}\n",
    "    for model_name, pipeline in pipelines.items():\n",
    "        print(f\"\\nüîß Processing with {model_name}...\")\n",
    "        try:\n",
    "            embeddings = pipeline(demo_texts)\n",
    "            all_embeddings[model_name] = embeddings\n",
    "            print(f\"‚úÖ Generated embeddings: {embeddings.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed: {e}\")\n",
    "    \n",
    "    # Analyze similarities\n",
    "    if len(all_embeddings) > 1:\n",
    "        print(f\"\\nüìä CROSS-MODEL SIMILARITY ANALYSIS\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        model_names = list(all_embeddings.keys())\n",
    "        for i, text_idx in enumerate([0, 1]):  # Analyze first two texts\n",
    "            print(f\"\\nüìù Text {text_idx+1}: \\\"{demo_texts[text_idx]}\\\"\")\n",
    "            \n",
    "            # Get embeddings for this text from all models\n",
    "            text_embeddings = {}\n",
    "            for model_name in model_names:\n",
    "                if model_name in all_embeddings:\n",
    "                    emb = all_embeddings[model_name]\n",
    "                    if len(emb.shape) > 1:\n",
    "                        text_embeddings[model_name] = emb[text_idx]\n",
    "                    else:\n",
    "                        text_embeddings[model_name] = emb\n",
    "            \n",
    "            # Compare embeddings between models\n",
    "            if len(text_embeddings) > 1:\n",
    "                print(f\"   üìè Embedding dimensions:\")\n",
    "                for model_name, emb in text_embeddings.items():\n",
    "                    print(f\"      {model_name}: {emb.shape} (norm: {np.linalg.norm(emb):.4f})\")\n",
    "    \n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763418ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_text_embedding_pipeline_results(results, save_dir='./text_embeddings'):\n",
    "    \"\"\"\n",
    "    Save text embedding pipeline results to files.\n",
    "    \n",
    "    Args:\n",
    "        results: Results from test_text_embeddings_with_examples()\n",
    "        save_dir: Directory to save results\n",
    "    \"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"üíæ Saving text embedding results to {save_path}\")\n",
    "    \n",
    "    successful_results = {name: data for name, data in results.items() if data.get('success', False)}\n",
    "    \n",
    "    if not successful_results:\n",
    "        print(\"‚ùå No successful results to save\")\n",
    "        return\n",
    "    \n",
    "    # Save individual model results\n",
    "    for model_name, data in successful_results.items():\n",
    "        model_dir = save_path / model_name.lower().replace('-', '_')\n",
    "        model_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Save embeddings\n",
    "        if 'single_embedding' in data:\n",
    "            np.save(model_dir / 'single_text_embedding.npy', data['single_embedding'])\n",
    "        if 'batch_embeddings' in data:\n",
    "            np.save(model_dir / 'batch_text_embeddings.npy', data['batch_embeddings'])\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'model_name': model_name,\n",
    "            'embedding_dim': data['embedding_dim'],\n",
    "            'single_shape': data['single_embedding'].shape if 'single_embedding' in data else None,\n",
    "            'batch_shape': data['batch_embeddings'].shape if 'batch_embeddings' in data else None\n",
    "        }\n",
    "        \n",
    "        import json\n",
    "        with open(model_dir / 'text_embedding_metadata.json', 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Saved {model_name} results to {model_dir}\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary = {\n",
    "        'successful_models': list(successful_results.keys()),\n",
    "        'failed_models': [name for name, data in results.items() if not data.get('success', False)],\n",
    "        'embedding_dimensions': {name: data['embedding_dim'] for name, data in successful_results.items()},\n",
    "        'timestamp': str(pd.Timestamp.now())\n",
    "    }\n",
    "    \n",
    "    with open(save_path / 'text_embedding_summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Saved summary to {save_path / 'text_embedding_summary.json'}\")\n",
    "    return save_path\n",
    "\n",
    "def custom_text_embedding_generator(text_input, models_to_use=None):\n",
    "    \"\"\"\n",
    "    Generate embeddings for custom text input using specified models.\n",
    "    \n",
    "    Args:\n",
    "        text_input: Single string or list of strings\n",
    "        models_to_use: List of model names to use (None for all available)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Model name -> embeddings mapping\n",
    "    \"\"\"\n",
    "    print(f\"üî§ Generating embeddings for custom text input...\")\n",
    "    \n",
    "    if isinstance(text_input, str):\n",
    "        print(f\"üìù Input text: \\\"{text_input[:100]}{'...' if len(text_input) > 100 else ''}\\\"\")\n",
    "    else:\n",
    "        print(f\"üìù Input: {len(text_input)} texts\")\n",
    "        for i, text in enumerate(text_input[:3]):  # Show first 3\n",
    "            print(f\"   {i+1}. \\\"{text[:80]}{'...' if len(text) > 80 else ''}\\\"\")\n",
    "        if len(text_input) > 3:\n",
    "            print(f\"   ... and {len(text_input)-3} more\")\n",
    "    \n",
    "    # Create pipelines\n",
    "    available_pipelines = create_all_text_pipelines()\n",
    "    \n",
    "    if models_to_use:\n",
    "        pipelines = {name: pipeline for name, pipeline in available_pipelines.items() \n",
    "                    if name in models_to_use}\n",
    "        missing = set(models_to_use) - set(available_pipelines.keys())\n",
    "        if missing:\n",
    "            print(f\"‚ö†Ô∏è  Requested models not available: {missing}\")\n",
    "    else:\n",
    "        pipelines = available_pipelines\n",
    "    \n",
    "    if not pipelines:\n",
    "        print(\"‚ùå No pipelines available\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"üîß Using models: {list(pipelines.keys())}\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    results = {}\n",
    "    for model_name, pipeline in pipelines.items():\n",
    "        print(f\"\\nüîÑ Generating embeddings with {model_name}...\")\n",
    "        try:\n",
    "            embeddings = pipeline(text_input)\n",
    "            results[model_name] = embeddings\n",
    "            \n",
    "            if isinstance(text_input, str):\n",
    "                print(f\"‚úÖ Generated embedding: {embeddings.shape} (norm: {np.linalg.norm(embeddings):.4f})\")\n",
    "            else:\n",
    "                print(f\"‚úÖ Generated embeddings: {embeddings.shape}\")\n",
    "                norms = [np.linalg.norm(emb) for emb in embeddings]\n",
    "                print(f\"   Norms: [{norms[0]:.4f}, {norms[1]:.4f}, ..., {norms[-1]:.4f}]\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed with {model_name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nüéâ Generated embeddings with {len(results)}/{len(pipelines)} models\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2223bd41",
   "metadata": {},
   "source": [
    "## üöÄ Execute Text Embedding Pipeline\n",
    "\n",
    "Run the text embedding pipeline to test all models with example materials science texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8462a14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Test text embedding pipelines with predefined examples\n",
    "print(\"=\" * 80)\n",
    "print(\"üß™ TESTING TEXT EMBEDDING PIPELINES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run comprehensive test\n",
    "test_results = test_text_embeddings_with_examples()\n",
    "\n",
    "# Save results\n",
    "if test_results:\n",
    "    saved_path = save_text_embedding_pipeline_results(test_results)\n",
    "    print(f\"\\nüíæ Results saved to: {saved_path}\")\n",
    "\n",
    "print(\"\\nüéâ Text embedding pipeline testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e1c472",
   "metadata": {},
   "source": [
    "## üéÆ Custom Text Embedding Examples\n",
    "\n",
    "Use these examples to generate embeddings for your own custom texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e233571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Example 1: Single custom text\n",
    "custom_text = \"Advanced polymer composite with carbon fiber reinforcement for aerospace applications\"\n",
    "\n",
    "print(\"üî§ Generating embeddings for custom text:\")\n",
    "print(f\"üìù Text: \\\"{custom_text}\\\"\")\n",
    "\n",
    "# Generate embeddings for all models\n",
    "custom_results = custom_text_embedding_generator(custom_text)\n",
    "\n",
    "# Display results\n",
    "if custom_results:\n",
    "    print(f\"\\nüìä EMBEDDING ANALYSIS:\")\n",
    "    for model_name, embedding in custom_results.items():\n",
    "        norm = np.linalg.norm(embedding)\n",
    "        print(f\"   üîó {model_name}: {embedding.shape} (norm: {norm:.4f})\")\n",
    "        print(f\"      Sample values: [{embedding[0]:.4f}, {embedding[1]:.4f}, ..., {embedding[-1]:.4f}]\")\n",
    "    \n",
    "    # Compare similarities if multiple models\n",
    "    if len(custom_results) > 1:\n",
    "        print(f\"\\nüîÑ CROSS-MODEL SIMILARITIES:\")\n",
    "        model_names = list(custom_results.keys())\n",
    "        for i in range(len(model_names)):\n",
    "            for j in range(i+1, len(model_names)):\n",
    "                model1, model2 = model_names[i], model_names[j]\n",
    "                emb1, emb2 = custom_results[model1], custom_results[model2]\n",
    "                \n",
    "                # Normalize embeddings for fair comparison\n",
    "                emb1_norm = emb1 / np.linalg.norm(emb1)\n",
    "                emb2_norm = emb2 / np.linalg.norm(emb2)\n",
    "                \n",
    "                similarity = np.dot(emb1_norm, emb2_norm)\n",
    "                print(f\"   {model1} ‚Üî {model2}: {similarity:.4f}\")\n",
    "else:\n",
    "    print(\"‚ùå No embeddings generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f838f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Example 2: Batch processing multiple texts\n",
    "batch_texts = [\n",
    "    \"Silicon carbide semiconductor for high-power electronics\",\n",
    "    \"Organic photovoltaic cell with improved efficiency\",\n",
    "    \"Magnetic nanoparticles for targeted drug delivery\",\n",
    "    \"Flexible conducting polymer for wearable devices\",\n",
    "    \"Ceramic matrix composite for high-temperature applications\"\n",
    "]\n",
    "\n",
    "print(\"üî§ Generating embeddings for batch of texts:\")\n",
    "print(f\"üìù Processing {len(batch_texts)} texts:\")\n",
    "for i, text in enumerate(batch_texts, 1):\n",
    "    print(f\"   {i}. {text}\")\n",
    "\n",
    "# Generate embeddings for all models\n",
    "batch_results = custom_text_embedding_generator(batch_texts)\n",
    "\n",
    "# Analyze batch results\n",
    "if batch_results:\n",
    "    print(f\"\\nüìä BATCH EMBEDDING ANALYSIS:\")\n",
    "    for model_name, embeddings in batch_results.items():\n",
    "        print(f\"\\nüîó {model_name}:\")\n",
    "        print(f\"   Shape: {embeddings.shape}\")\n",
    "        \n",
    "        # Compute statistics\n",
    "        norms = [np.linalg.norm(emb) for emb in embeddings]\n",
    "        print(f\"   Norms: min={min(norms):.4f}, max={max(norms):.4f}, mean={np.mean(norms):.4f}\")\n",
    "        \n",
    "        # Compute pairwise similarities within batch\n",
    "        if embeddings.shape[0] > 1:\n",
    "            # Normalize embeddings\n",
    "            normalized_embs = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "            similarity_matrix = np.dot(normalized_embs, normalized_embs.T)\n",
    "            \n",
    "            # Show similarity statistics\n",
    "            off_diagonal = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]\n",
    "            print(f\"   Similarities: min={off_diagonal.min():.4f}, max={off_diagonal.max():.4f}, mean={off_diagonal.mean():.4f}\")\n",
    "            \n",
    "            # Show most similar pair\n",
    "            max_sim_idx = np.unravel_index(np.argmax(similarity_matrix - np.eye(len(similarity_matrix))), \n",
    "                                         similarity_matrix.shape)\n",
    "            max_sim_val = similarity_matrix[max_sim_idx]\n",
    "            print(f\"   Most similar: Text {max_sim_idx[0]+1} ‚Üî Text {max_sim_idx[1]+1} (similarity: {max_sim_val:.4f})\")\n",
    "            \n",
    "    # Compare embedding patterns across models\n",
    "    if len(batch_results) > 1:\n",
    "        print(f\"\\nüîÑ CROSS-MODEL COMPARISON:\")\n",
    "        model_names = list(batch_results.keys())\n",
    "        \n",
    "        # Compare embedding dimensions\n",
    "        dims = {name: embs.shape[1] for name, embs in batch_results.items()}\n",
    "        print(f\"   Embedding dimensions: {dims}\")\n",
    "        \n",
    "        # Compare first text across models\n",
    "        print(f\"\\n   First text across models:\")\n",
    "        first_text_embeddings = {name: embs[0] for name, embs in batch_results.items()}\n",
    "        \n",
    "        for i, model1 in enumerate(model_names):\n",
    "            for model2 in model_names[i+1:]:\n",
    "                emb1 = first_text_embeddings[model1]\n",
    "                emb2 = first_text_embeddings[model2]\n",
    "                \n",
    "                # Normalize for comparison\n",
    "                emb1_norm = emb1 / np.linalg.norm(emb1)\n",
    "                emb2_norm = emb2 / np.linalg.norm(emb2)\n",
    "                \n",
    "                similarity = np.dot(emb1_norm, emb2_norm)\n",
    "                print(f\"      {model1} ‚Üî {model2}: {similarity:.4f}\")\n",
    "else:\n",
    "    print(\"‚ùå No batch embeddings generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f885e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Example 3: Interactive custom text input\n",
    "# You can modify these texts to test with your own materials\n",
    "\n",
    "def quick_text_embedding_demo():\n",
    "    \"\"\"Quick demo function for testing custom texts.\"\"\"\n",
    "    print(\"üéÆ QUICK TEXT EMBEDDING DEMO\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Define your custom texts here\n",
    "    your_texts = [\n",
    "        \"PUT YOUR CUSTOM TEXT HERE\",\n",
    "        \"Quantum dots for display applications\",\n",
    "        \"Biodegradable polymer for medical implants\",\n",
    "        \"Superconducting material at room temperature\"\n",
    "    ]\n",
    "    \n",
    "    # You can also test with a single text\n",
    "    single_test_text = \"Graphene oxide membrane for water filtration\"\n",
    "    \n",
    "    print(f\"üìù Testing single text:\")\n",
    "    print(f\"   \\\"{single_test_text}\\\"\")\n",
    "    \n",
    "    # Test single text\n",
    "    single_results = custom_text_embedding_generator(single_test_text, \n",
    "                                                   models_to_use=['CLIPP-SciBERT', 'MobileCLIP'])  # Specify models if desired\n",
    "    \n",
    "    if single_results:\n",
    "        print(f\"\\n‚úÖ Generated embeddings for {len(single_results)} models\")\n",
    "        for model, emb in single_results.items():\n",
    "            print(f\"   {model}: {emb.shape} (norm: {np.linalg.norm(emb):.4f})\")\n",
    "    \n",
    "    print(f\"\\nüìù Testing batch texts:\")\n",
    "    # Filter out placeholder text\n",
    "    real_texts = [text for text in your_texts if not text.startswith(\"PUT YOUR\")]\n",
    "    \n",
    "    if real_texts:\n",
    "        batch_results = custom_text_embedding_generator(real_texts)\n",
    "        \n",
    "        if batch_results:\n",
    "            print(f\"\\n‚úÖ Generated batch embeddings for {len(batch_results)} models\")\n",
    "            for model, embs in batch_results.items():\n",
    "                print(f\"   {model}: {embs.shape}\")\n",
    "        \n",
    "        return single_results, batch_results\n",
    "    else:\n",
    "        print(\"   (Modify 'your_texts' list above to test custom inputs)\")\n",
    "        return single_results, None\n",
    "\n",
    "# Run the demo\n",
    "demo_single, demo_batch = quick_text_embedding_demo()\n",
    "\n",
    "print(f\"\\nüí° TIP: To test your own texts:\")\n",
    "print(f\"   1. Modify the 'your_texts' list in the cell above\")\n",
    "print(f\"   2. Replace 'single_test_text' with your text\")\n",
    "print(f\"   3. Re-run the cell\")\n",
    "print(f\"   4. Use 'models_to_use' parameter to test specific models only\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
