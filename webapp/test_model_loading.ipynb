{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9514d9b",
   "metadata": {},
   "source": [
    "# MaterialVision Model Loading Demo\n",
    "\n",
    "This notebook demonstrates how to load and use the different vision-language models available in the MaterialVision project:\n",
    "\n",
    "- **CLIPP-SciBERT**: CLIPP model with SciBERT text encoder\n",
    "- **CLIPP-DistilBERT**: CLIPP model with DistilBERT text encoder  \n",
    "- **MobileCLIP**: Apple's MobileCLIP model\n",
    "- **BLIP**: Salesforce's BLIP model for image-text retrieval\n",
    "\n",
    "Each model has its own loading function that handles checkpoint loading, device placement, and provides a consistent interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf21e5",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5d74e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import importlib.util\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "# Since we're already in the webapp directory, we can import models.py directly\n",
    "# No need to add paths since models.py is in the same directory\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0e46b",
   "metadata": {},
   "source": [
    "## 2. Load Functions from External Files\n",
    "\n",
    "Now let's import the model loading functions from the `models.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09e3dd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding to path: /home/jipengsun/MaterialVision/models/CLIPP_allenai\n",
      "‚úÖ Successfully imported CLIPP SciBERT\n",
      "Adding to path: /home/jipengsun/MaterialVision/models/CLIPP_bert\n",
      "‚úÖ Successfully imported CLIPP DistilBERT\n",
      "Adding to path: /home/jipengsun/MaterialVision/models/Apple_MobileCLIP\n",
      "‚úÖ Successfully imported MobileCLIP\n",
      "Adding to path: /home/jipengsun/MaterialVision/models/Salesforce\n",
      "‚úÖ Successfully imported BLIP\n",
      "‚úÖ Successfully imported model loading functions:\n",
      "  - load_clipp_scibert\n",
      "  - load_clipp_distilbert\n",
      "  - load_mobileclip\n",
      "  - load_blip\n",
      "‚úÖ Successfully imported CLIPP SciBERT\n",
      "Adding to path: /home/jipengsun/MaterialVision/models/CLIPP_bert\n",
      "‚úÖ Successfully imported CLIPP DistilBERT\n",
      "Adding to path: /home/jipengsun/MaterialVision/models/Apple_MobileCLIP\n",
      "‚úÖ Successfully imported MobileCLIP\n",
      "Adding to path: /home/jipengsun/MaterialVision/models/Salesforce\n",
      "‚úÖ Successfully imported BLIP\n",
      "‚úÖ Successfully imported model loading functions:\n",
      "  - load_clipp_scibert\n",
      "  - load_clipp_distilbert\n",
      "  - load_mobileclip\n",
      "  - load_blip\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Import model loading functions from models.py\n",
    "    from models import (\n",
    "        load_clipp_scibert,\n",
    "        load_clipp_distilbert, \n",
    "        load_mobileclip,\n",
    "        load_blip\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Successfully imported model loading functions:\")\n",
    "    print(\"  - load_clipp_scibert\")\n",
    "    print(\"  - load_clipp_distilbert\")\n",
    "    print(\"  - load_mobileclip\") \n",
    "    print(\"  - load_blip\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing model functions: {e}\")\n",
    "    print(\"Make sure you're running this notebook from the MaterialVision root directory\")\n",
    "    print(\"and that the webapp/models.py file exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d53a7c",
   "metadata": {},
   "source": [
    "## 3. Call Loaded Functions with Sample Data\n",
    "\n",
    "Let's check for available checkpoints and demonstrate loading each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79c501ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ clipp_scibert: ../models/CLIPP_allenai/checkpoints/best_clipp.pth\n",
      "‚úÖ clipp_distilbert: ../models/CLIPP_bert/checkpoints/best_clipp_bert.pth\n",
      "‚úÖ mobileclip: ../models/Apple_MobileCLIP/checkpoints/best_clipp_apple.pth\n",
      "‚úÖ blip: ../models/Salesforce/checkpoints_blip/best_blip.pth\n",
      "\n",
      "Found 4 available model checkpoints.\n"
     ]
    }
   ],
   "source": [
    "# Define checkpoint paths (relative to webapp directory, go up one level to access models)\n",
    "checkpoint_paths = {\n",
    "    'clipp_scibert': '../models/CLIPP_allenai/checkpoints/best_clipp.pth',\n",
    "    'clipp_distilbert': '../models/CLIPP_bert/checkpoints/best_clipp_bert.pth', \n",
    "    'mobileclip': '../models/Apple_MobileCLIP/checkpoints/best_clipp_apple.pth',\n",
    "    'blip': '../models/Salesforce/checkpoints_blip/best_blip.pth'\n",
    "}\n",
    "\n",
    "# Check which checkpoints exist\n",
    "available_models = {}\n",
    "for model_name, path in checkpoint_paths.items():\n",
    "    full_path = Path(path)\n",
    "    if full_path.exists():\n",
    "        available_models[model_name] = str(full_path)\n",
    "        print(f\"‚úÖ {model_name}: {path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {model_name}: {path} (not found)\")\n",
    "\n",
    "print(f\"\\nFound {len(available_models)} available model checkpoints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9552a0",
   "metadata": {},
   "source": [
    "### 3.1 Load CLIPP-SciBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc1bc244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIPP-SciBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 23:18:23,539 INFO: Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)\n",
      "2025-11-09 23:18:23,581 INFO: [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CLIPP-SciBERT model loaded successfully!\n",
      "   Model device: cuda:0\n",
      "   Tokenizer type: BertTokenizerFast\n",
      "   Dataset type: ImageTextDataset\n",
      "sample input_ids: tensor([ 102,  158,  504,  170, 1240,  170, 3471,  244,  205,  244,  103,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "sample attention_mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Text embedding shape: torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "if 'clipp_scibert' in available_models:\n",
    "    try:\n",
    "        print(\"Loading CLIPP-SciBERT model...\")\n",
    "        clipp_scibert_model, clipp_scibert_tokenizer, clipp_scibert_dataset = load_clipp_scibert(\n",
    "            checkpoint_path=available_models['clipp_scibert'],\n",
    "            device=str(device)\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ CLIPP-SciBERT model loaded successfully!\")\n",
    "        print(f\"   Model device: {next(clipp_scibert_model.parameters()).device}\")\n",
    "        print(f\"   Tokenizer type: {type(clipp_scibert_tokenizer).__name__}\")\n",
    "        print(f\"   Dataset type: {type(clipp_scibert_dataset).__name__}\")        \n",
    "        # Test tokenization\n",
    "        sample_text = \"The chemical formula is UGe2Pt2. The mbj_bandgap value is 0.0.\"\n",
    "        caption, input_ids, attention_mask = clipp_scibert_dataset.prepare_caption(sample_text)\n",
    "        print(f\"sample input_ids: {input_ids}\")\n",
    "        print(f\"sample attention_mask: {attention_mask}\")\n",
    "\n",
    "        # Test text embedding\n",
    "        txt_emb = clipp_scibert_model.get_text_features(input_ids.view(1,-1).to(device), attention_mask.view(1,-1).to(device))\n",
    "        print(f\"Text embedding shape: {txt_emb.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading CLIPP-SciBERT: {e}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  CLIPP-SciBERT checkpoint not available, skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620bfb5b",
   "metadata": {},
   "source": [
    "### 3.2 Load CLIPP-DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ecd665e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIPP-DistilBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 23:18:40,711 INFO: Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "2025-11-09 23:18:40,756 INFO: [timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CLIPP-DistilBERT model loaded successfully!\n",
      "   Model device: cuda:0\n",
      "   Tokenizer type: DistilBertTokenizer\n",
      "   Dataset type: ImageTextDataset\n",
      "Text embedding shape: torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "if 'clipp_distilbert' in available_models:\n",
    "    try:\n",
    "        print(\"Loading CLIPP-DistilBERT model...\")\n",
    "        clipp_distilbert_model, clipp_distilbert_tokenizer, clipp_distilbert_dataset = load_clipp_distilbert(\n",
    "            checkpoint_path=available_models['clipp_distilbert'],\n",
    "            device=str(device)\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ CLIPP-DistilBERT model loaded successfully!\")\n",
    "        print(f\"   Model device: {next(clipp_distilbert_model.parameters()).device}\")\n",
    "        print(f\"   Tokenizer type: {type(clipp_distilbert_tokenizer).__name__}\")\n",
    "        print(f\"   Dataset type: {type(clipp_distilbert_dataset).__name__}\")\n",
    "\n",
    "        # Test tokenization\n",
    "        sample_text = \"The chemical formula is UGe2Pt2. The mbj_bandgap value is 0.0.\"\n",
    "        caption, input_ids, attention_mask = clipp_distilbert_dataset.prepare_caption(sample_text)\n",
    "        embeddings = clipp_distilbert_model.get_text_features(input_ids.view(1,-1).to(device), attention_mask.view(1,-1).to(device))\n",
    "        print(f\"Text embedding shape: {embeddings.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading CLIPP-DistilBERT: {e}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  CLIPP-DistilBERT checkpoint not available, skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488de851",
   "metadata": {},
   "source": [
    "### 3.3 Load MobileCLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e48ee3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading BLIP embeddings from: embeddings/val_df_with_embeddings_apple.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load BLIP embeddings from saved pickle file\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the path to the BLIP embeddings\n",
    "blip_embeddings_path = Path('./embeddings/val_df_with_embeddings_apple.pkl')\n",
    "\n",
    "print(f\"üîÑ Loading BLIP embeddings from: {blip_embeddings_path}\")\n",
    "\n",
    "if blip_embeddings_path.exists():\n",
    "    # Load the pickle file\n",
    "    with open(blip_embeddings_path, 'rb') as f:\n",
    "        blip_embeddings_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba6fea42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(blip_embeddings_data.iloc[0][\"val_txt_embs\"][0][0], embeddings.cpu()[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e961f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 00:20:59,847 INFO: Loaded MobileCLIP-S2 model config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MobileCLIP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 00:21:01,495 INFO: Loading pretrained MobileCLIP-S2 weights (datacompdr).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MobileCLIP model loaded successfully!\n",
      "   Model device: cuda:0\n",
      "   Tokenizer type: <class 'open_clip.tokenizer.SimpleTokenizer'>\n",
      "   Dataset type: ImageTextDataset\n",
      "Text embedding shape: torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "if 'mobileclip' in available_models:\n",
    "    try:\n",
    "        print(\"Loading MobileCLIP model...\")\n",
    "        mobileclip_model, mobileclip_tokenizer, mobileclip_dataset = load_mobileclip(\n",
    "            checkpoint_path=available_models['mobileclip'],\n",
    "            device=str(device)\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ MobileCLIP model loaded successfully!\")\n",
    "        print(f\"   Model device: {next(mobileclip_model.parameters()).device}\")\n",
    "        print(f\"   Tokenizer type: {type(mobileclip_tokenizer)}\")\n",
    "        print(f\"   Dataset type: {type(mobileclip_dataset).__name__}\")\n",
    "\n",
    "        # Test tokenization (MobileCLIP uses different tokenization)\n",
    "        sample_text = \"The chemical formula is LiGeS. The  mbj_bandgap value is 0.0.\"\n",
    "        caption, text_tokens = mobileclip_dataset.prepare_caption(sample_text)\n",
    "        embeddings = mobileclip_model.get_text_features(text_tokens.to(device))\n",
    "        print(f\"Text embedding shape: {embeddings.shape}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading MobileCLIP: {e}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  MobileCLIP checkpoint not available, skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eba7318",
   "metadata": {},
   "source": [
    "### 3.4 Load BLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d588ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BLIP model...\n",
      "‚úÖ BLIP model loaded successfully!\n",
      "   Model device: cuda:0\n",
      "   Processor type: BlipProcessor\n",
      "   Dataset type: ImageTextDataset\n",
      "Text embedding shape: torch.Size([1, 256])\n",
      "‚úÖ BLIP model loaded successfully!\n",
      "   Model device: cuda:0\n",
      "   Processor type: BlipProcessor\n",
      "   Dataset type: ImageTextDataset\n",
      "Text embedding shape: torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "if 'blip' in available_models:\n",
    "    try:\n",
    "        print(\"Loading BLIP model...\")\n",
    "        blip_model, blip_processor, blip_dataset = load_blip(\n",
    "            checkpoint_path=available_models['blip'],\n",
    "            device=str(device)\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ BLIP model loaded successfully!\")\n",
    "        print(f\"   Model device: {next(blip_model.parameters()).device}\")\n",
    "        print(f\"   Processor type: {type(blip_processor).__name__}\")\n",
    "        print(f\"   Dataset type: {type(blip_dataset).__name__}\")\n",
    "        \n",
    "        # Test text processing\n",
    "        sample_text = \"The chemical formula is LiGeS. The  mbj_bandgap value is 0.0.\"\n",
    "        caption, input_ids, attention_mask = blip_dataset.prepare_caption(sample_text)\n",
    "        embeddings = blip_model.get_text_features(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n",
    "        print(f\"Text embedding shape: {embeddings.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading BLIP: {e}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  BLIP checkpoint not available, skipping...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
