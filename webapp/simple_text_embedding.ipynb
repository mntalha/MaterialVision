{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0877d809",
   "metadata": {},
   "source": [
    "# MaterialVision: Embedding Pipeline\n",
    "\n",
    "This notebook provides a streamlined approach to:\n",
    "1. Load models and datasets\n",
    "2. Load validation data and create embeddings\n",
    "3. Generate embeddings for custom text samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a6a0a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jipengsun/.conda/envs/clipp/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/jipengsun/.conda/envs/clipp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding to path: /home/jipengsun/MaterialVision/models/CLIPP_allenai\n",
      "‚úÖ Successfully imported CLIPP SciBERT\n",
      "Adding to path: /home/jipengsun/MaterialVision/models/CLIPP_bert\n",
      "‚úÖ Successfully imported CLIPP SciBERT\n",
      "Adding to path: /home/jipengsun/MaterialVision/models/CLIPP_bert\n",
      "‚úÖ Successfully imported CLIPP DistilBERT\n",
      "Adding to path: /home/jipengsun/MaterialVision/models/Apple_MobileCLIP\n",
      "‚úÖ Successfully imported MobileCLIP\n",
      "Adding to path: /home/jipengsun/MaterialVision/models/Salesforce\n",
      "‚ùå Error importing BLIP: No module named 'models.CLIPP_bert'; 'models' is not a package\n",
      "‚úÖ Imports completed\n",
      "üìã Available models: ['CLIPP-SciBERT', 'CLIPP-DistilBERT', 'MobileCLIP', 'BLIP']\n",
      "‚úÖ Successfully imported CLIPP DistilBERT\n",
      "Adding to path: /home/jipengsun/MaterialVision/models/Apple_MobileCLIP\n",
      "‚úÖ Successfully imported MobileCLIP\n",
      "Adding to path: /home/jipengsun/MaterialVision/models/Salesforce\n",
      "‚ùå Error importing BLIP: No module named 'models.CLIPP_bert'; 'models' is not a package\n",
      "‚úÖ Imports completed\n",
      "üìã Available models: ['CLIPP-SciBERT', 'CLIPP-DistilBERT', 'MobileCLIP', 'BLIP']\n"
     ]
    }
   ],
   "source": [
    "# Essential imports and setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "# Import model functions\n",
    "from models import (\n",
    "    load_clipp_scibert,\n",
    "    load_clipp_distilbert, \n",
    "    load_mobileclip,\n",
    "    load_blip\n",
    ")\n",
    "\n",
    "# Available models\n",
    "MODELS = ['CLIPP-SciBERT', 'CLIPP-DistilBERT', 'MobileCLIP', 'BLIP']\n",
    "\n",
    "print(\"‚úÖ Imports completed\")\n",
    "print(f\"üìã Available models: {MODELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccb4a2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CSV path: ../data/alpaca_mbj_bandgap_test.csv\n",
      "Validation CSV exists: True\n",
      "‚úÖ Loaded validation data with 1000 samples\n",
      "   Columns: ['instruction', 'input', 'response', 'id', 'image']\n",
      "‚úÖ Loaded validation data with 1000 samples\n",
      "   Columns: ['instruction', 'input', 'response', 'id', 'image']\n"
     ]
    }
   ],
   "source": [
    "# Define validation data path\n",
    "VAL_CSV = Path('../data/alpaca_mbj_bandgap_test.csv')\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f\"Validation CSV path: {VAL_CSV}\")\n",
    "print(f\"Validation CSV exists: {VAL_CSV.exists()}\")\n",
    "# Load validation data\n",
    "if VAL_CSV.exists():\n",
    "    val_df = pd.read_csv(VAL_CSV)\n",
    "    print(f\"‚úÖ Loaded validation data with {len(val_df)} samples\")\n",
    "    print(f\"   Columns: {list(val_df.columns)}\")\n",
    "else:\n",
    "    print(f\"‚ùå Validation CSV not found at {VAL_CSV}\")\n",
    "    val_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c416de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text embedding function ready\n"
     ]
    }
   ],
   "source": [
    "def create_text_embeddings(model_name, texts):\n",
    "    \"\"\"\n",
    "    Create  embeddings for given sample using specified model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model to use\n",
    "        texts: Single text string or list of texts\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of embeddings\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    print(f\"üîÑ Loading {model_name} model...\")\n",
    "    \n",
    "    try:\n",
    "        # Define device\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # Load model based on type\n",
    "        if model_name == 'CLIPP-SciBERT':\n",
    "            checkpoint_path = '../models/CLIPP_allenai/checkpoints/best_clipp.pth'\n",
    "            model, tokenizer, _ = load_clipp_scibert(checkpoint_path, device)\n",
    "            \n",
    "            embeddings = []\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for text in texts:\n",
    "                    tokens = tokenizer(text, padding=True, truncation=True, \n",
    "                                     return_tensors=\"pt\", max_length=512).to(device)\n",
    "                    text_features = model.get_text_features(tokens['input_ids'], tokens['attention_mask'])\n",
    "                    embeddings.append(text_features.cpu().numpy())\n",
    "        \n",
    "        elif model_name == 'CLIPP-DistilBERT':\n",
    "            checkpoint_path = '../models/CLIPP_bert/checkpoints/best_clipp_bert.pth'\n",
    "            model, tokenizer, _ = load_clipp_distilbert(checkpoint_path, device)\n",
    "            \n",
    "            embeddings = []\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for text in texts:\n",
    "                    tokens = tokenizer(text, padding=True, truncation=True, \n",
    "                                     return_tensors=\"pt\", max_length=512).to(device)\n",
    "                    text_features = model.get_text_features(tokens['input_ids'], tokens['attention_mask'])\n",
    "                    embeddings.append(text_features.cpu().numpy())\n",
    "        \n",
    "        elif model_name == 'MobileCLIP':\n",
    "            import open_clip\n",
    "            checkpoint_path = '../models/Apple_MobileCLIP/checkpoints/best_clipp_apple.pth'\n",
    "            model, tokenizer, _ = load_mobileclip(checkpoint_path, device)\n",
    "            \n",
    "            embeddings = []\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for text in texts:\n",
    "                    tokens = open_clip.tokenize([text]).to(device)\n",
    "                    text_features = model.get_text_features(tokens)\n",
    "                    embeddings.append(text_features.cpu().numpy())\n",
    "        \n",
    "        elif model_name == 'BLIP':\n",
    "            checkpoint_path = '../models/Salesforce/checkpoints_blip/best_blip.pth'\n",
    "            model, processor, _ = load_blip(checkpoint_path, device)\n",
    "            \n",
    "            embeddings = []\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for text in texts:\n",
    "                    inputs = processor(text=[text], return_tensors=\"pt\", \n",
    "                                     padding=True, truncation=True, max_length=512).to(device)\n",
    "                    text_embeds = model.get_text_features(**inputs)\n",
    "                    text_embeds = F.normalize(text_embeds, p=2, dim=1)\n",
    "                    embeddings.append(text_embeds.cpu().numpy())\n",
    "        \n",
    "        result = np.vstack(embeddings) if len(embeddings) > 1 else embeddings[0]\n",
    "        print(f\"‚úÖ Generated embeddings: {result.shape}\")\n",
    "        return result.squeeze() if len(texts) == 1 else result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {model_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Text embedding function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dee1db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting validation data processing...\n",
      "üìä Processing 10 validation samples\n",
      "\n",
      "üîÑ Processing CLIPP-SciBERT...\n",
      "üîÑ Loading CLIPP-SciBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 19:57:08,204 INFO: Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)\n",
      "2025-11-09 19:57:08,245 INFO: [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2025-11-09 19:57:08,245 INFO: [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated embeddings: (10, 256)\n",
      "‚úÖ CLIPP-SciBERT: (10, 256)\n",
      "\n",
      "üîÑ Processing CLIPP-DistilBERT...\n",
      "üîÑ Loading CLIPP-DistilBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 19:57:11,817 INFO: Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "2025-11-09 19:57:11,860 INFO: [timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2025-11-09 19:57:11,860 INFO: [timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2025-11-09 19:57:13,044 INFO: Loaded MobileCLIP-S2 model config.\n",
      "2025-11-09 19:57:13,044 INFO: Loaded MobileCLIP-S2 model config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated embeddings: (10, 256)\n",
      "‚úÖ CLIPP-DistilBERT: (10, 256)\n",
      "\n",
      "üîÑ Processing MobileCLIP...\n",
      "üîÑ Loading MobileCLIP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 19:57:14,404 INFO: Loading pretrained MobileCLIP-S2 weights (datacompdr).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated embeddings: (10, 256)\n",
      "‚úÖ MobileCLIP: (10, 256)\n",
      "\n",
      "üîÑ Processing BLIP...\n",
      "üîÑ Loading BLIP model...\n",
      "‚úÖ Generated embeddings: (10, 256)\n",
      "‚úÖ BLIP: (10, 256)\n",
      "\n",
      "üéâ Completed processing for 4 models\n",
      "‚úÖ Generated embeddings: (10, 256)\n",
      "‚úÖ BLIP: (10, 256)\n",
      "\n",
      "üéâ Completed processing for 4 models\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for validation dataset\n",
    "def process_validation_data(models_to_use=None):\n",
    "    \"\"\"\n",
    "    Process validation dataset and create embeddings for all models.\n",
    "    \"\"\"\n",
    "    if val_df is None:\n",
    "        print(\"‚ùå No validation data available\")\n",
    "        return {}\n",
    "    \n",
    "    models_to_process = models_to_use or MODELS\n",
    "    results = {}\n",
    "    \n",
    "    # Sample first 10 texts for demo (change as needed)\n",
    "    sample_texts = val_df['input'].head(10).tolist()\n",
    "    print(f\"üìä Processing {len(sample_texts)} validation samples\")\n",
    "    \n",
    "    for model_name in models_to_process:\n",
    "        print(f\"\\nüîÑ Processing {model_name}...\")\n",
    "        embeddings = create_text_embeddings(model_name, sample_texts)\n",
    "        \n",
    "        if embeddings is not None:\n",
    "            results[model_name] = {\n",
    "                'embeddings': embeddings,\n",
    "                'texts': sample_texts,\n",
    "                'shape': embeddings.shape\n",
    "            }\n",
    "            print(f\"‚úÖ {model_name}: {embeddings.shape}\")\n",
    "        else:\n",
    "            print(f\"‚ùå {model_name}: Failed\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run validation processing\n",
    "print(\"üöÄ Starting validation data processing...\")\n",
    "validation_results = process_validation_data()\n",
    "print(f\"\\nüéâ Completed processing for {len(validation_results)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a110151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing with custom material descriptions...\n",
      "üìù Input text: \"Silicon carbide semiconductor with high thermal conductivity for power electronics...\")\n",
      "\n",
      "üîÑ CLIPP-SciBERT...\n",
      "üîÑ Loading CLIPP-SciBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 19:58:55,817 INFO: Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)\n",
      "2025-11-09 19:58:55,869 INFO: [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2025-11-09 19:58:55,869 INFO: [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated embeddings: (1, 256)\n",
      "‚úÖ Shape: (256,), Norm: 1.0\n",
      "\n",
      "üîÑ CLIPP-DistilBERT...\n",
      "üîÑ Loading CLIPP-DistilBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 19:58:59,370 INFO: Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "2025-11-09 19:58:59,414 INFO: [timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2025-11-09 19:58:59,414 INFO: [timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2025-11-09 19:59:00,544 INFO: Loaded MobileCLIP-S2 model config.\n",
      "2025-11-09 19:59:00,544 INFO: Loaded MobileCLIP-S2 model config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated embeddings: (1, 256)\n",
      "‚úÖ Shape: (256,), Norm: 1.0\n",
      "\n",
      "üîÑ MobileCLIP...\n",
      "üîÑ Loading MobileCLIP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 19:59:01,872 INFO: Loading pretrained MobileCLIP-S2 weights (datacompdr).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated embeddings: (1, 256)\n",
      "‚úÖ Shape: (256,), Norm: 1.0\n",
      "\n",
      "üîÑ BLIP...\n",
      "üîÑ Loading BLIP model...\n",
      "‚úÖ Generated embeddings: (1, 256)\n",
      "‚úÖ Shape: (256,), Norm: 1.0000001192092896\n",
      "üìù Processing 3 texts\n",
      "\n",
      "üîÑ CLIPP-SciBERT...\n",
      "üîÑ Loading CLIPP-SciBERT model...\n",
      "‚úÖ Generated embeddings: (1, 256)\n",
      "‚úÖ Shape: (256,), Norm: 1.0000001192092896\n",
      "üìù Processing 3 texts\n",
      "\n",
      "üîÑ CLIPP-SciBERT...\n",
      "üîÑ Loading CLIPP-SciBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 19:59:10,597 INFO: Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)\n",
      "2025-11-09 19:59:10,648 INFO: [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2025-11-09 19:59:10,648 INFO: [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated embeddings: (3, 256)\n",
      "‚úÖ Shape: (3, 256), Norm: [np.float32(1.0), np.float32(1.0), np.float32(0.99999994)]\n",
      "\n",
      "üîÑ CLIPP-DistilBERT...\n",
      "üîÑ Loading CLIPP-DistilBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 19:59:13,680 INFO: Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "2025-11-09 19:59:13,721 INFO: [timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2025-11-09 19:59:13,721 INFO: [timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2025-11-09 19:59:14,814 INFO: Loaded MobileCLIP-S2 model config.\n",
      "2025-11-09 19:59:14,814 INFO: Loaded MobileCLIP-S2 model config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated embeddings: (3, 256)\n",
      "‚úÖ Shape: (3, 256), Norm: [np.float32(1.0), np.float32(1.0), np.float32(1.0)]\n",
      "\n",
      "üîÑ MobileCLIP...\n",
      "üîÑ Loading MobileCLIP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 19:59:16,350 INFO: Loading pretrained MobileCLIP-S2 weights (datacompdr).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated embeddings: (3, 256)\n",
      "‚úÖ Shape: (3, 256), Norm: [np.float32(1.0), np.float32(1.0), np.float32(1.0)]\n",
      "\n",
      "üîÑ BLIP...\n",
      "üîÑ Loading BLIP model...\n",
      "‚úÖ Generated embeddings: (3, 256)\n",
      "‚úÖ Shape: (3, 256), Norm: [np.float32(0.99999994), np.float32(1.0), np.float32(1.0)]\n",
      "\n",
      "üéâ Custom text testing completed!\n",
      "‚úÖ Generated embeddings: (3, 256)\n",
      "‚úÖ Shape: (3, 256), Norm: [np.float32(0.99999994), np.float32(1.0), np.float32(1.0)]\n",
      "\n",
      "üéâ Custom text testing completed!\n"
     ]
    }
   ],
   "source": [
    "# Test with custom text samples\n",
    "def test_custom_text(text_input, models_to_use=None):\n",
    "    \"\"\"\n",
    "    Generate embeddings for custom text using all or specified models.\n",
    "    \n",
    "    Args:\n",
    "        text_input: Single text or list of texts\n",
    "        models_to_use: List of model names (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of model_name -> embeddings\n",
    "    \"\"\"\n",
    "    if isinstance(text_input, str):\n",
    "        print(f\"üìù Input text: \\\"{text_input[:100]}...\\\")\")\n",
    "    else:\n",
    "        print(f\"üìù Processing {len(text_input)} texts\")\n",
    "    \n",
    "    models_to_process = models_to_use or MODELS\n",
    "    results = {}\n",
    "    \n",
    "    for model_name in models_to_process:\n",
    "        print(f\"\\nüîÑ {model_name}...\")\n",
    "        embeddings = create_text_embeddings(model_name, text_input)\n",
    "        \n",
    "        if embeddings is not None:\n",
    "            results[model_name] = embeddings\n",
    "            norm = np.linalg.norm(embeddings) if embeddings.ndim == 1 else [np.linalg.norm(emb) for emb in embeddings]\n",
    "            print(f\"‚úÖ Shape: {embeddings.shape}, Norm: {norm}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage with custom texts\n",
    "print(\"\\nüß™ Testing with custom material descriptions...\")\n",
    "\n",
    "# Test single text\n",
    "custom_text = \"Silicon carbide semiconductor with high thermal conductivity for power electronics\"\n",
    "single_results = test_custom_text(custom_text)\n",
    "\n",
    "# Test multiple texts\n",
    "custom_texts = [\n",
    "    \"Graphene-based composite for energy storage applications\",\n",
    "    \"Perovskite solar cell with enhanced stability\",\n",
    "    \"Titanium alloy with superior mechanical properties\"\n",
    "]\n",
    "batch_results = test_custom_text(custom_texts)\n",
    "\n",
    "print(\"\\nüéâ Custom text testing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0814acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results (optional)\n",
    "def save_results(results, filename='embedding_results.pkl'):\n",
    "    \"\"\"\n",
    "    Save embedding results to pickle file.\n",
    "    \"\"\"\n",
    "    save_path = Path('./outputs') \n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    filepath = save_path / filename\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    \n",
    "    print(f\"üíæ Results saved to {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "# Save validation and custom results\n",
    "all_results = {\n",
    "    'validation_results': validation_results,\n",
    "    'single_text_results': single_results,\n",
    "    'batch_text_results': batch_results\n",
    "}\n",
    "\n",
    "save_results(all_results, 'simple_embedding_results.pkl')\n",
    "\n",
    "print(\"\\nüìä Summary:\")\n",
    "print(f\"‚úÖ Validation models processed: {len(validation_results)}\")\n",
    "print(f\"‚úÖ Single text models: {len(single_results)}\")\n",
    "print(f\"‚úÖ Batch text models: {len(batch_results)}\")\n",
    "print(\"\\nüéØ Pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d0d8f0",
   "metadata": {},
   "source": [
    "## Quick Usage Examples\n",
    "\n",
    "### Text Embeddings\n",
    "```python\n",
    "# Generate embeddings for any text\n",
    "my_text = \"Your material description here\"\n",
    "results = test_custom_text(my_text, models_to_use=['CLIPP-SciBERT', 'MobileCLIP'])\n",
    "\n",
    "# Generate text embeddings directly\n",
    "text_embeddings = create_text_embeddings('CLIPP-SciBERT', \"Your text here\")\n",
    "```\n",
    "\n",
    "### Image Embeddings\n",
    "```python\n",
    "# Generate embeddings for images\n",
    "image_paths = [\"/path/to/image1.jpg\", \"/path/to/image2.jpg\"]\n",
    "image_embeddings = create_image_embeddings('CLIPP-SciBERT', image_paths)\n",
    "\n",
    "# Single image\n",
    "single_image_embedding = create_image_embeddings('CLIPP-SciBERT', \"/path/to/image.jpg\")\n",
    "```\n",
    "\n",
    "### Multimodal Embeddings\n",
    "```python\n",
    "# Generate both text and image embeddings\n",
    "results = test_custom_multimodal(\n",
    "    text_input=[\"Material description 1\", \"Material description 2\"],\n",
    "    image_paths=[\"/path/to/image1.jpg\", \"/path/to/image2.jpg\"],\n",
    "    models_to_use=['CLIPP-SciBERT']\n",
    ")\n",
    "\n",
    "# Text and images together\n",
    "multimodal_embeddings = create_multimodal_embeddings(\n",
    "    model_name='CLIPP-SciBERT',\n",
    "    texts=[\"Silicon carbide semiconductor\"],\n",
    "    image_paths=[\"/path/to/sic_image.jpg\"]\n",
    ")\n",
    "```\n",
    "\n",
    "### Process Validation Data\n",
    "```python\n",
    "# Process validation data with specific models (text + images)\n",
    "val_results = process_validation_data(models_to_use=['CLIPP-SciBERT'], include_images=True)\n",
    "\n",
    "# Text only\n",
    "val_results_text = process_validation_data(models_to_use=['CLIPP-SciBERT'], include_images=False)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
